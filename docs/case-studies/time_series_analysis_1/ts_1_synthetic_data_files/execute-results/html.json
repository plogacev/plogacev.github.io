{
  "hash": "52bcb7b5f7ca3f24ea4d2116d941c099",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Generating Synthetic Order Volume\"\nauthor: \"Pavel Logačev\"\ndate: \"r Sys.Date()\"\nformat:\n  html:\n    toc: true\n    code-fold: true\n    code-tools: true\n    df-print: paged\nengine: knitr\neditor: visual\n---\n\n\n\n# Generating Synthetic Order Volume\n\n## Introduction\n\nUnderstanding real-world sales patterns often requires modeling specific aspects of the data which can be obscured by factors that are of no theoretical interest. For instance, the effects of price sensitivity may be obscured by seasonal patterns, seasonal patterns may be partially obscured by one-off events, effects of consumer confidence, or by long-term trends, among others. By constructing a synthetic dataset with well-defined components, we can test whether a model can effectively recover these hidden relationshop.\n\nThis notebook creates synthetic sales data for an online store and explains the logic behind its generation. The goal is to simulate a realistic sales time series incorporating latent seasonality (both weekly and yearly), latent growth over time, as well as influences of unmodeled factors via a random walk. The generated data can be used for testing time series models that aim to uncover some of these latent structures.\n\nKey aspects of our data:\n\n-   **Latent Growth.** Sales increase gradually, accelerating after a certain point, and then saturate.\n-   **Yearly Seasonality.** Sales vary throughout the year, with a peak during certain periods (e.g., summer or holiday seasons).\n-   **Weekly Seasonality.** A periodic pattern emerges within each week (e.g., higher sales on weekends).\n-   **Random Walk Noise.** Unmodeled variations and external shocks are captured through a random walk process, ensuring realistic fluctuations.\n\n## 1. Import Required Libraries And Define Functions\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\nimport polars as pl\nimport numpy as np\nimport pandas as pd\nfrom plotnine import ggplot, aes, geom_line, labs, theme_minimal, theme_bw, scale_x_continuous, scale_x_discrete, scale_x_datetime\n```\n:::\n\n::: {.cell}\n\n```{.python .cell-code}\n# Creates a simple plot using plotnine\ndef plot_function(x, y, title, xlab, ylab):\n    # Convert x to numpy array\n    x = np.array(x)\n    \n    # Check if y is a callable function\n    if callable(y):\n        # If y is a function, apply it to x and create a DataFrame\n        df = pd.DataFrame({\"x\": x, \"y\": y(x)})\n    else:\n        # If y is not a function, create a DataFrame directly\n        df = pd.DataFrame({\"x\": x, \"y\": y})        \n\n    # Create the plot using ggplot\n    plot = (ggplot(df, aes(x=\"x\", y=\"y\")) + geom_line() + labs(title=title, x=xlab, y=ylab) + theme_bw())\n    \n    return plot\n```\n:::\n\n\n\n## 2. Data Generation Process\n\nIn the initial steps, we will create a number of independent components that contribute to the price independently. In principle, the way to interpret them is 'this is what sales would look like over the time frame considered is all else remained equal'.\n\n### 2.1. Create the Time Series\n\nA daily time series is generated starting from June 1, 2021, up to the present day. This ensures sufficient data points to analyze trends and seasonality.\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\n# set the random seed\nnp.random.seed(42)\n\n# define date range\nstart_date = \"2021-06-01\"\nend_date = pd.Timestamp.today().strftime(\"%Y-%m-%d\")\ndate_range = pd.date_range(start=start_date, end=end_date, freq=\"D\")\n\n# generate time index\ndays_since_start = np.arange(len(date_range))\n```\n:::\n\n\n\n### 2.2. Long-Term Growth Curve\n\nLong-term sales growth is modeled as a modified logistic function, representing an initial slow growth phase, followed by acceleration, and eventual saturation. This function ensures that sales begin near zero, increase slowly at first, then accelerate before stabilizing.\n\n$$ f(t)  = L \\cdot (1 + exp(-k \\cdot (t - t_0)))^{(-1/v)} $$\n\nParameters: - **L.** Upper bound on sales (saturation level). - **k.** Growth rate. - **x_0.** Inflection point (where acceleration peaks). - **v.** Asymmetry parameter (v \\< 1 slower growth to the right of **x0**, 0 \\< v \\< 1 slower growth to the left of **x0**)\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\n# Logistic function parameters\nL = 1      # Upper saturation limit\nk = 0.0125 # Growth rate\nt0 = 1100  # Inflection point\nv = 5      # Asymmetry parameter (v > 1 shifts right, 0 < v < 1 shifts left)\n\n# Compute the logistic growth curve\ngrowth_fn = lambda t: L * (1 + v * np.exp(-k * (t - t0)))**(-1/v)\ngrowth = growth_fn(days_since_start)\n\np = plot_function(x=date_range, y=growth, title=\"Logistic Growth Over Time\", xlab=\"Days Since Start\", ylab=\"Sales Factor\")\n_ = p.draw(show=True)\n```\n\n::: {.cell-output-display}\n![](ts_1_synthetic_data_files/figure-html/unnamed-chunk-4-1.png){width=614}\n:::\n:::\n\n\n\n### 2.3. Yearly Seasonal Pattern\n\nYearly seasonality is introduced using a scaled cosine transformations of the day of the year. This captures periodic effects such as, in this case, increased summer sales. This function is scaled to oscillate between 0.8 and 1.2 over the course of a year, which, when multiplied with the average sales function futher down will decrease winter sales by up to $20\\%$, and increase summer sales by up to $20\\%$.\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\n# Yearly seasonality\nyearly_seasonality_fn = lambda day: 1 + 0.2 * np.cos(2 * np.pi * day / 365.25 - np.pi)\nyearly_seasonality = yearly_seasonality_fn(date_range.day_of_year)\n\np = plot_function(x=range(0,366), y=yearly_seasonality_fn, title=\"Yearly Seasonality\", xlab=\"Day of the Year\", ylab=\"Sales Factor\")\n_ = p.draw(show=True)\n```\n\n::: {.cell-output-display}\n![](ts_1_synthetic_data_files/figure-html/unnamed-chunk-5-3.png){width=614}\n:::\n:::\n\n\n\n### 2.4. Weekly Seasonal Pattern\n\nWeekly seasonality is also modeled a scaled cosine transformations of the day of the week. In this case, we model a drop in sales, primarily on Thursday-Friday. This function is scaled to oscillate between 0.9 and 1.1 over the course of the week, which, when multiplied with the average sales function futher will change sales by $\\pm 10\\%$.\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\n# Weekly seasonality\nweekly_seasonality_fn = lambda day: 1 + 0.1 * np.cos(2 * np.pi * day / 7)\nweekly_seasonality = weekly_seasonality_fn(date_range.weekday)\n\np = plot_function(x=range(0,7), y=weekly_seasonality_fn, title=\"Weekly Seasonality\", xlab=\"Day of the Week\", ylab=\"Sales Factor\") \n_ = p.draw(show=True)\n```\n\n::: {.cell-output-display}\n![](ts_1_synthetic_data_files/figure-html/unnamed-chunk-6-5.png){width=614}\n:::\n:::\n\n\n\n### 2.5. Combining Growth and Seasonality\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\nsales = np.array(growth) * np.array( yearly_seasonality ) * np.array( weekly_seasonality )\nbreaks = [pd.Timestamp(d) for d in [\"2017-01-01\", \"2019-01-01\", \"2021-01-01\", \"2023-01-01\", \"2025-01-01\"]]\n\np = plot_function(x=date_range, y=sales, title=\"Growth + Seasonality\", xlab=\"Date\", ylab=\"Sales Factor\") + scale_x_datetime(breaks = breaks)\n_ = p.draw(show=True)\n```\n\n::: {.cell-output-display}\n![](ts_1_synthetic_data_files/figure-html/unnamed-chunk-7-7.png){width=614}\n:::\n:::\n\n\n\n### 2.6. Random Walk: Unmodeled Influences and External Shocks\n\nA random walk is used to simulate external influences and unpredictable variations. This component accounts for factors not explicitly modeled, such as promotions, economic shifts, or changes in popularity, or influences of competitors. The random walk is centered (mean zero) to ensure it does not systematically bias the trend. This ensures that the long-term sales trajectory remains driven by the logistic growth component rather than arbitrary drift. This does not amount to any sort of assumption about the data-generating process in a more realistic scenario. This is done strictly to maintain interpretability in the bringing together of the different parts of the synthetic demand.\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\nnp.random.seed(441)\n\nrandom_walk = np.cumsum(np.random.normal(scale=.015, size=len(date_range)))\ncentered_random_walk = random_walk - np.mean(random_walk)\n\np = plot_function(x=date_range, y = centered_random_walk, title=\"Random Walk Component\", xlab=\"Date\", ylab=\"Latent Sales\") + scale_x_datetime(breaks = breaks)\n_ = p.draw(show=True)\n```\n\n::: {.cell-output-display}\n![](ts_1_synthetic_data_files/figure-html/unnamed-chunk-8-9.png){width=614}\n:::\n:::\n\n\n\nLet's visualize the growth together with the random walk for future reference, because they will be estimated as one component in the model in the next notebook.\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\ngrowth_plus_rw = np.exp( np.log(growth) + centered_random_walk)\np = plot_function(x=date_range, y = growth_plus_rw, title=\"Growth + Random Walk Component\", xlab=\"Date\", ylab=\"Latent Sales\") + scale_x_datetime(breaks = breaks)\n_ = p.draw(show=True)\n```\n\n::: {.cell-output-display}\n![](ts_1_synthetic_data_files/figure-html/unnamed-chunk-9-11.png){width=614}\n:::\n:::\n\n\n\nThe random walk is combined with the sales pattern created so far in log-space in order to ensure that the effects are multiplicative. This aligns with real-world sales data, where sales fluctuations are typically proportional rather than absolute. This is also a simple way of preventing sales from dropping below 0.\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\nsales_with_random_component = np.exp( np.log(sales) + centered_random_walk)\np = plot_function(x=date_range, y = sales_with_random_component, title=\"Growth + Seasonality + Random Walk\", xlab=\"Date\", ylab=\"Latent Sales\") + scale_x_datetime(breaks = breaks)\n_ = p.draw(show=True)\n```\n\n::: {.cell-output-display}\n![](ts_1_synthetic_data_files/figure-html/unnamed-chunk-10-13.png){width=614}\n:::\n:::\n\n\n\n### 2.7. Effect of Price\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\ndef sample_log_price_change(n, p, min_delta, max_delta):\n    \"\"\"Sample n values from a mixture of:\n    - 0 with probability p\n    - Uniform(min_delta, max_delta) with probability (1-p)\n    \"\"\"\n    zero_mask = np.random.rand(n) < p  # Boolean mask for zeros\n    delta_log_price_nonzero = np.random.uniform(min_delta, max_delta, n)  # Sample from Uniform(a, b)\n    \n    # Combine: replace values with 0 where zero_mask is True\n    delta_log_price = np.where(zero_mask, 0, delta_log_price_nonzero)\n    return delta_log_price\n```\n:::\n\n::: {.cell}\n\n```{.python .cell-code}\ndelta_log_price = [0.0]*len(date_range)\ndelta_log_price[150] = .1\ndelta_log_price[300] = .1\ndelta_log_price[500] = -.15\ndelta_log_price[750] = .1\ndelta_log_price[1000] = .1\ndelta_log_price[1200] = .05\np = plot_function(x=date_range, y = np.cumsum(delta_log_price), title=\"Difference in log price to baseline\", xlab=\"Date\", ylab=\"Δ log(price)\") + scale_x_datetime(breaks = breaks)\n_ = p.draw(show=True)\n```\n\n::: {.cell-output-display}\n![](ts_1_synthetic_data_files/figure-html/unnamed-chunk-12-15.png){width=614}\n:::\n:::\n\n::: {.cell}\n\n```{.python .cell-code}\nprice_base = 20\nlog_price = np.log(price_base) + np.cumsum(delta_log_price)\np = plot_function(x=date_range, y = np.exp( log_price ), title=\"Product Price\", xlab=\"Date\", ylab=\"Price\") + scale_x_datetime(breaks = breaks)\n_ = p.draw(show=True)\n```\n\n::: {.cell-output-display}\n![](ts_1_synthetic_data_files/figure-html/unnamed-chunk-13-17.png){width=614}\n:::\n:::\n\n::: {.cell}\n\n```{.python .cell-code}\nelasticity = -1.4\nsales_with_price_effect = np.exp( np.log(sales_with_random_component) + elasticity * (log_price - np.mean(log_price)) )\np = plot_function(x=date_range, y = sales_with_price_effect, title=\"\", xlab=\"Day of the Week\", ylab=\"Latent Sales\") + scale_x_datetime(breaks = breaks)\n_ = p.draw(show=True)\n```\n\n::: {.cell-output-display}\n![](ts_1_synthetic_data_files/figure-html/unnamed-chunk-14-19.png){width=614}\n:::\n:::\n\n\n\n### 2.8. Scaled Sales\n\nAt this point, we scale the expected sales to a more realistic range for actual sales.\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\nmax_sales = 200 # scale the sales to a more realistic range\nscale_factor = max_sales / sales_with_price_effect.max()\nsales_scaled = scale_factor * sales_with_price_effect \n\np = plot_function(x=date_range, y = sales_scaled, title=\"Scaled Latent Sales\", xlab=\"Date\", ylab=\"Latent Sales\") + scale_x_datetime(breaks = breaks)\n_ = p.draw(show=True)\n```\n\n::: {.cell-output-display}\n![](ts_1_synthetic_data_files/figure-html/unnamed-chunk-15-21.png){width=614}\n:::\n:::\n\n\n\n## 3. Realized Sales\n\nWhat we constructed until now are the expected sales $\\lambda$ for each day. We realize them for each day $i$ by drawing them from a Poisson distribution with parameter $\\lambda_i$. This approach ensures that while the underlying sales structure is generated smoothly, the final dataset exhibits realistic integer sales values with appropriate stochastic variation.\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\nsales_realized = np.random.poisson(lam=sales_scaled)\np = plot_function(x=date_range, y = sales_realized, title=\"Realized Sales\", xlab=\"Date\", ylab=\"Latent Sales\") + scale_x_datetime(breaks = breaks)\n_ = p.draw(show=True)\n```\n\n::: {.cell-output-display}\n![](ts_1_synthetic_data_files/figure-html/unnamed-chunk-16-23.png){width=614}\n:::\n:::\n\n\n\n## 4. Sanity-Check\n\nHere, we check that the composition of the sales time series is as expected. We'll estimate regression coefficients for all the components. All should be 1, with the exception of the effect of price, which should equal the specified elastictiy. Please note though, that although all components are additive in log-space, their contributions are not equal, since they are scaled differently.\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\nimport statsmodels.api as sm\n\n# fit poisson glm\nX = pd.DataFrame({\n    'intercept': np.ones(len(date_range)),  # Intercept\n    'centered_random_walk': centered_random_walk,\n    'weekly_seasonality': np.log(weekly_seasonality),\n    'yearly_seasonality': np.log(yearly_seasonality),\n    'growth': np.log(growth),\n    'log_price': log_price\n})\n\ny = sales_realized\npoisson_model = sm.GLM(y, X, family=sm.families.Poisson()).fit()\nsummary = poisson_model.summary()\nprint( summary )\n```\n:::\n\n\n\n## 5. Save Sales\n\nHaving instantiated the sales time series, we save the latent and realized sales in CSV format.\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\ndf = pl.DataFrame({\n    \"date\": date_range.astype(str).tolist(),\n    \"log_price\": log_price,  \n    \"sales_latent\": sales_scaled,\n    \"sales\": sales_realized\n})\ndf.write_csv(\"sales_synthetic.csv\")\n```\n:::\n\n::: {.cell}\n\n```{.python .cell-code}\nimport pickle\n\nwith open(\"sim_parameters.pkl\", \"wb\") as f:\n    wdays = list(range(0, 7))\n    weekly_seasonality = [weekly_seasonality_fn(wday) for wday in wdays]\n    yearly_seasonality = yearly_seasonality_fn(date_range.day_of_year)\n    sim_parameters = date_range, growth, growth_plus_rw, scale_factor, wdays, weekly_seasonality, yearly_seasonality\n    pickle.dump(sim_parameters, f)\n```\n:::\n",
    "supporting": [
      "ts_1_synthetic_data_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-in-header": [
        "<link href=\"../../site_libs/pagedtable-1.1/css/pagedtable.css\" rel=\"stylesheet\" />\n<script src=\"../../site_libs/pagedtable-1.1/js/pagedtable.js\"></script>\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}