[
  {
    "objectID": "case-studies/pricedist_elicitation/price_distribution_elicitation.html",
    "href": "case-studies/pricedist_elicitation/price_distribution_elicitation.html",
    "title": "Price Distribution Elicitation from an LLM",
    "section": "",
    "text": "Codelibrary(tidyverse)\nlibrary(magrittr)\nlibrary(ggplot2)\nlibrary(scales)\nlibrary(reticulate)\nCodeimport os\nimport polars as pl\nimport re\nimport ast\nfrom tqdm import tqdm\nimport math\nimport itertools\nimport openai\nimport pickle\n\nfrom dotenv import load_dotenv\n\nfrom openai import OpenAI\nfrom joblib import Memory\n\n# \ntmp = load_dotenv()\n\n# save the results from the LLM model to a cache\nmemory_summary = Memory(location=\"cache_summary\", verbose=0)\nmemory_pricing = Memory(location=\"cache_pricing\", verbose=0)"
  },
  {
    "objectID": "case-studies/pricedist_elicitation/price_distribution_elicitation.html#summary",
    "href": "case-studies/pricedist_elicitation/price_distribution_elicitation.html#summary",
    "title": "Price Distribution Elicitation from an LLM",
    "section": "Summary",
    "text": "Summary\nThis notebook presents a two-stage pipeline for eliciting plausible online retail price ranges for consumer products using large language models (LLMs). It supports pricing analysts in making initial pricing decisions when historical data or comparable products are unavailable or unreliable."
  },
  {
    "objectID": "case-studies/pricedist_elicitation/price_distribution_elicitation.html#motivation-and-approach",
    "href": "case-studies/pricedist_elicitation/price_distribution_elicitation.html#motivation-and-approach",
    "title": "Price Distribution Elicitation from an LLM",
    "section": "Motivation and Approach",
    "text": "Motivation and Approach\nWhen pricing a new product for the first time, or re-evaluating an existing product‚Äôs pricing, pricing analysts are often flying blind. In many cases, there is no information about competitor pricing, limited or no sales history, and little variability in past price points. As a result, prices can‚Äôt be based on competitive benchmarks or estimated elasticities. Yet a price still needs to be set.\nIn these situations, the goal is not precision: it is to make a well-reasoned initial decision. The price should be high enough to protect margin, low enough to be competitive, and aligned with how a reasonably informed customer would perceive the product: what it does, what it‚Äôs made of, who it‚Äôs for, and how the brand signals quality or positioning.\nTo support this task, the notebook demonstrates a two-stage pipeline using large language models (LLMs) for eliciting reasonable price ranges. The method is applied to consumer product data from Mercado Libre sourced from here:\n\nProduct Attribute Extraction GPT-3.5-turbo is used to extract a structured set of pricing-relevant attributes ‚Äî such as product type, components, materials, intended use, and product tier ‚Äî from unstructured titles and descriptions. The output is a standardized schema of factual product properties.\nPrice Range Elicitation GPT-4o is then used on attributes to elicit a plausible retail price range ‚Äî minimum, typical, and maximum ‚Äî conditioned on regional market context (e.g., country, currency, and year).\n\nThe two-step approach has several advantages over one-shot elicitation:\n\nExplicit control over inputs: By decoupling attribute extraction from price judgment, we can precisely define which aspects of the product are considered ‚Äî such as function, materials, target user, or technical features ‚Äî rather than letting the model base its pricing on irrelevant or misleading text. This reduces hallucinations and improves consistency.\nCost efficiency: Structured product summaries can be generated using a much smaller model and inexpensive model than the one used for eliciting price points, such as a local mt5-small or GPT-3.5-turbo, with only the pricing step, which benefits from broader and more recent market knowledge, being handled by GPT-4o. This separation helps control token usage and reduces overall API costs.\nTransparency and auditability: Having an explicit intermediate representation allows analysts to inspect what the model ‚Äúsaw‚Äù before making a pricing judgment. This makes the process easier to debug, validate, or even override with human input if needed.\n\nBoth stages are implemented with batched processing, caching (joblib.Memory), and fallback logic to ensure reliability and scalability. The resulting price ranges can be used to support pricing diagnostics, detect potential over- or underpricing, and provide a structured starting point for analyst review.\nData\nThe dataset (sourced from here) contains \\(5,859\\) product listings from Mercado Libre, a major Latin American e-commerce platform. The data was likely obtained via web scraping. The presence of both original and discounted prices suggests that pricing metadata was parsed directly from the listing structure, possibly through structured HTML extraction.\nIn the present notebook, we use the following columns:\n\n\nproduct_title: a free-text product title from the listing (in Spanish)\n\nproduct_description: a potentially marketing description (in Spanish)\n\nprice_usd: the listed retail price, converted to USD\n\nprice_discounted_usd: the discounted price (if applicable), also converted to USD\n\nproduct_url: a direct link to the product listing (mainly used for debugging)\n\nproduct_id: a synthetic ID assigned during preprocessing (mainly used for debugging)\n\n\nCodeproducts = pl.read_csv(\"./data/mercado_libre_products_cleaned.csv\")\nproducts = (products\n            .rename({'Product': 'product_title', 'Description': 'product_description',\n                      'Product URL': 'product_url',\n                      'MXN': 'price_mxn', 'USD': 'price_usd', \n                      'Sale Price USD': 'price_discounted_usd'})\n            .with_columns( pl.arange(0, products.height).alias('product_id') )\n            .select([ 'product_id', 'product_title', 'product_description', 'price_usd', 'price_discounted_usd', 'product_url'])\n           )\n\n\n\nLet‚Äôs take a look at the data frame with the product information.\n\n\nCodepy$products$to_pandas()\n\n\n\nNext, we connect to OpenAI.\n\n\nCodeclient = OpenAI(api_key=os.environ[\"OPENAI_API_KEY\"])\n\n\nProduct Attribute Extraction\n\nBelow is the prompt that serves to extract the pricing-related features from the product title and product description. In principle, the product images can be included as well.\n\n\nCodeprompt_template_product_properties = \"\"\"\nYou are a product analyst.\n\nExtract pricing-relevant attributes from each of the following products.\n\nFocus exclusively on the **actual nature and use** of the product ‚Äî its type, components, materials, and intended use ‚Äî as well as the intended target group.  \nDo not be influenced by stylistic choices, promotional language, or verbosity in the original title or description.\n\nFor each product, extract the following information:\n- Brand ‚Äì if applicable\n- Core Product Name without any unnecessary qualifications, if one is available; in English if internationally known\n- Product Type ‚Äì general category or function of the item\n- Key Components ‚Äì any major physical or electronic subcomponents (e.g., power adapter, filter unit)\n- Distinguishing Product Features ‚Äì physical or functional traits not found in all comparable products (e.g., \"72h battery life\", \"IP68 water resistance\", \"remote control\", \"8-core CPU\")\n- Variant Features ‚Äì options that vary across listings or versions, such as size, color, voltage, memory, etc.\n- Functional Tier ‚Äì classify the product as \"budget\", \"standard\", or \"premium\" based on: objective technical features, material quality, and positioning within the brand‚Äôs lineup or market segment.\n    - Exclude marketing phrasing and minor stylistic differences (e.g., LED clock, color options, decorative packaging).\n    - A product is premium only if it clearly surpasses alternatives in core functionality, quality, and typical price level.\n- Materials ‚Äì main materials used (e.g., plastic, steel, leather)\n- Intended Use ‚Äì main use case (e.g., \"home cleaning\", \"child transport\", \"audio playback\")\n- Target User ‚Äì intended end-user (e.g., \"toddlers\", \"pet owners\", \"DIY hobbyists\")\n- Quantity or Unit Size ‚Äì volume, weight, or unit count (in metric units if applicable)\n- Year of Make ‚Äì if known or can be inferred\n- Regulatory Certifications or Standards ‚Äì if applicable (e.g., CE, FDA, NOM)\n- Likely Group Marketed To ‚Äì broad audience the product seems designed for (e.g., \"fitness enthusiasts\", \"tech-savvy users\")\n- Likely Income Bracket ‚Äì infer \"low\", \"middle\", or \"high\" based on the nature of the product and its expected price range, not on superficial style\n\n\nProducts:\n{product_blocks}\n\nInstructions:\nReturn a syntactically valid Python list of dicts. Each dict must correspond to one product, and must include the following fields in this exact order:\n\n`product_id, brand, product_name, product_type, components, product_features, variant_features, functional_tier, materials, intended_use, target_user, quantity, year_make, regulatory_certifications, marketed_to, marketed_to_income`\n\n- When a property is unknown or unclear, and an educated guess is not possible, return `'-'` for that field\n- Do **not** use vague summaries or marketing phrasing ‚Äî extract or infer concrete factual content only\n- Use **European metrics** for quantity/unit size (e.g., ml, cm, g); convert if necessary\n- Do not include explanations, markdown, or formatting beyond the list of dicts\n\"\"\".strip()\n\n\n\nHere, we define the the logic of the product feature extraction.\n\n\nCodedef make_product_blocks(product_list):\n    blocks = []\n    for i, product in enumerate(product_list, 1):\n        blocks.append(f\"\"\"Product {i}:\n                          &lt;ID&gt;\n                          {product['product_id']}\n                          &lt;/ID&gt;\n                          &lt;TITLE&gt;\n                          {product['product_title']}\n                          &lt;/TITLE&gt;\n                          &lt;DESCRIPTION&gt;\n                          {product['product_description']}\n                          &lt;/DESCRIPTION&gt;\n                      \"\"\")\n    return \"\\n\\n\".join(blocks).strip()\n\n@memory_summary.cache(ignore=[\"client\"])\ndef summarize_product_descriptions_batch(client, product_list):\n        \"\"\" \"\"\"\n        product_blocks = make_product_blocks(product_list)\n        \n        prompt = prompt_template_product_properties.format(\n            product_blocks=product_blocks\n        )\n\n        try:\n          response = client.chat.completions.create(\n              model=\"gpt-3.5-turbo\",\n              messages=[{\"role\": \"user\", \"content\": prompt}],\n              temperature=0,\n          )\n          \n          reply = response.choices[0].message.content\n          reply = re.sub(r\"^```(?:python)?\\s*|\\s*```$\", \"\", reply.strip(), flags=re.IGNORECASE)\n          reply = ast.literal_eval(reply)\n\n        except:\n          if batch_size == 1:\n              raise\n          new_batch_size = math.floor(batch_size/2)\n          #print(f\"new batch size: {new_batch_size}\")\n          #if \"maximum context length\" in str(e):\n          return summarize_product_descriptions(client, product_list, batch_size=new_batch_size)\n        \n        # to-do: make sure all products were returned, and re-request the missing ones if any were missing\n\n        return reply\n\n\n@memory_summary.cache(ignore=[\"client\"])\ndef summarize_product_descriptions(client, product_list, batch_size=10):\n    \"\"\" \"\"\"\n    num_batches = math.ceil(len(product_list) / batch_size)\n    product_list_it = iter(product_list)\n    \n    results = []\n    for _ in tqdm(range(num_batches), desc=\"Retrieving price ranges\"):\n        product_batch = list(itertools.islice(product_list_it, batch_size))\n        result_batch = summarize_product_descriptions_batch(client, product_batch)\n        results.extend(result_batch)\n\n    return results\n\n\n\nLet‚Äôs extract pricing-related product attributes now, caching them during and after retrieval.\n\n\nCodeproduct_list = products[['product_id', 'product_title', 'product_description']].to_dicts()\n\nfname_product_summaries = \"product_summaries_chatgtp35_turbo.pkl\"\nif not os.path.exists(fname_product_summaries):\n    product_summaries = summarize_product_descriptions(client, product_list, batch_size = 10)\n    \n    with open(fname_product_summaries, \"wb\") as f:\n      pickle.dump(product_summaries, f)\n      \nelse:\n    with open(fname_product_summaries, \"rb\") as f:\n        product_summaries = pickle.load(f)\n\n\n\nCode# Coerce lists to strings for compatibility\nproduct_summaries_uniform = [\n    {k: \", \".join(v) if isinstance(v, list) else v for k, v in row.items()}\n    for row in product_summaries\n]\n\nproduct_summaries_uniform_df = pl.DataFrame(product_summaries_uniform)\n\n\n\nLet‚Äôs take a look at the results of this feature extraction.\n\n\nCodepy$product_summaries_uniform_df$to_pandas()\n\n\nPrice Range Elicitation\n\nCode# to-do: do mention that we're looking for *list* prices, and not any kind of discounted or promotional prices -- or maybe elicit both types of prices (sometimes the 'discounted price' is the actual price)\nprompt_template_price_range = \"\"\"\nYou are a pricing assistant.\n\nEstimate the typical **online retail** price range for each of the following products, based on publicly available prices in {region}, as close to the year {year} as possible. Prioritize prices from local online platforms and regional e-commerce sites. If local data is scarce, use prices from the most **geographically or economically comparable regions** for which prices are available.\n- Prioritize sources with high traffic and strong market influence (e.g., Amazon, local online supermarkets, major regional e-commerce platforms).\n- Reflect everyday consumer pricing ‚Äî exclude promotional or bulk prices.\n\nAll prices must be reported in **{currency}**. If source prices are in a different currency, adjust to {currency} using appropriate historical exchange rates and contextual knowledge.\n\nFocus exclusively on the **actual nature and use** of the product, and the target group ‚Äî its type, components, materials, and intended use.\n- Ignore stylistic or marketing choices in the title or description (e.g., exaggerated adjectives, description length, or promotional phrasing).\n- Prioritize functional and categorical cues, such as:\n  - What is the product?\n  - What is it made of?\n  - Who is it for (e.g. child vs. adult, consumer vs. professional)?\n  - How is it typically used?\n\nFor each product, estimate:\n- The **lowest plausible price** ‚Äî the lowest price a typical retailer might charge for this item, excluding outliers or defective goods.\n- The **highest plausible price** ‚Äî the upper bound of reasonable retail pricing, excluding rare luxury versions or bundles.\n- The **most typical price** ‚Äî the price point at which the product is most commonly sold (median or mode).\n\nProducts:\n{product_summary}\n\nInstructions:\nReturn a syntacticly valid python list of lists, each in the following format:\n[product_id, lowest_price, highest_price, typical_price]\n\nDo not include explanations, citations, or formatting beyond this structure.\n\"\"\".strip()\n\n\n\nCode@memory_pricing.cache(ignore=[\"client\"])\ndef retrieve_price_ranges_batch(client, product_summary, currency, year, region):\n    \"\"\" \"\"\"\n    prompt = prompt_template_price_range.format(\n        product_summary=pprint.pformat(product_summary),\n        currency=currency,\n        year=year,\n        region=region,\n    )\n    #prompt = re.sub(r'\\s+', ' ', prompt)\n    #print(prompt)\n    \n    response = client.chat.completions.create(\n        model=\"gpt-4o\",\n        messages=[{\"role\": \"user\", \"content\": prompt}],\n        temperature=0,\n    )\n    \n    reply = response.choices[0].message.content\n    reply = re.sub(r\"^```(?:python)?\\s*|\\s*```$\", \"\", reply.strip(), flags=re.IGNORECASE)\n    estimates = ast.literal_eval(reply)\n    \n    results = []\n    for est in estimates:\n        cur_result = { 'product_id':  est[0], 'lower': est[1], 'upper': est[2], 'typical': est[3] }\n        results.append( cur_result )\n    \n    # to-do: make sure all products were returned, and re-request the missing ones if any were missing\n    \n    return results\n\n@memory_pricing.cache(ignore=[\"client\"])\ndef retrieve_price_ranges(client, product_summary, currency, year, region, batch_size=10):\n    \"\"\" \"\"\"\n    num_batches = math.ceil(len(product_summary) / batch_size)\n    product_list_it = iter(product_summary)\n    \n    results = []\n    for _ in tqdm(range(num_batches), desc=\"Retrieving price ranges\"):\n        product_batch = list(itertools.islice(product_list_it, batch_size))\n        result_batch = retrieve_price_ranges_batch(client, product_batch, currency, year, region)\n        results.extend(result_batch)\n\n    return results\n\n\n\nCodecurrency = \"USD\"\nyear = 2025\nregion = \"Mexico\"\n\n\n\nCodefname_estimates = \"estimates_chatgtp4o.pkl\"\n\nif not os.path.exists(fname_estimates):\n    estimates = retrieve_price_ranges(client, product_summaries_uniform, currency, year, region, batch_size = 15)\n    \n    with open(fname_estimates, \"wb\") as f:\n      pickle.dump(estimates, f)\n      \nelse:\n    with open(fname_estimates, \"rb\") as f:\n        estimates = pickle.load(f)"
  },
  {
    "objectID": "case-studies/pricedist_elicitation/price_distribution_elicitation.html#evaluation",
    "href": "case-studies/pricedist_elicitation/price_distribution_elicitation.html#evaluation",
    "title": "Price Distribution Elicitation from an LLM",
    "section": "Evaluation",
    "text": "Evaluation\n\nCodeimport pickle\n\nwith open(\"product_summaries_chatgtp35_turbo.pkl\", \"rb\") as f:\n    product_summaries = pickle.load(f)\n\n\n\nCodeestimates = pl.DataFrame( estimates )\nestimates = estimates.with_columns( pl.col(\"product_id\").cast(pl.Int128) ) \nproduct_prices = products.join( estimates, how = \"left\", on = \"product_id\" )\nproduct_prices_pd = product_prices.to_pandas()\n\n\n\nCodeproduct_prices &lt;- py$product_prices$to_pandas() %&gt;% filter(!is.na(typical))\n\n\nAccuracy on List Prices\n\nThe plot below shows the actual product list prices in USD (x-axis), along with the elicited prices (y-axis). Both axes are on the log-scale. The vertical bars correspond to price range. Even though there are significant deviations, we can see that the model produces price estimates close to the observed list price.\n\n\nCodeproduct_prices %&gt;% filter(!is.na(typical)) %&gt;% \n  ggplot(aes(x = price_usd, y = typical)) +\n  geom_point(alpha=0.3) +\n  geom_errorbar(aes(ymin=lower, ymax=upper), alpha=0.3, width=0) +\n  geom_abline(intercept = 0, slope = 1, linetype = \"dashed\", color = \"red\") +\n  #scale_x_continuous(limits = c(0, 4000)) + scale_y_continuous(limits = c(0, 4000)) +\n  theme_bw() +\n  scale_x_log10(labels = label_dollar(prefix = \"$\", accuracy = 1)) + \n  scale_y_log10(labels = label_dollar(prefix = \"$\", accuracy = 1)) +\n  xlab(\"Actual list price in USD (log-scale)\") +\n  ylab(\"Elicited price in USD (log-scale)\")\n\n\nAccuracy on Discounted Prices\n\nThis plot shows the discounted prices in USD (x-axis), along with the elicited prices (y-axis). This plot shows that elicited prices tend to be higher than observed discounted prices.\n\n\nCodeproduct_prices %&gt;% filter(!is.na(typical)) %&gt;% \n  ggplot(aes(x = price_discounted_usd, y = typical)) +\n  geom_point(alpha=0.3) +\n  geom_errorbar(aes(ymin=lower, ymax=upper), alpha=0.3, width=0) +\n  geom_abline(intercept = 0, slope = 1, linetype = \"dashed\", color = \"red\") +\n  #scale_x_continuous(limits = c(0, 4000)) + scale_y_continuous(limits = c(0, 4000)) +\n  theme_bw() +\n  scale_x_log10(labels = label_dollar(prefix = \"$\", accuracy = 1)) + \n  scale_y_log10(labels = label_dollar(prefix = \"$\", accuracy = 1)) +\n  xlab(\"Actual discounted price in USD (log-scale)\") +\n  ylab(\"Elicited price in USD (log-scale)\")"
  },
  {
    "objectID": "case-studies/pricedist_elicitation/price_distribution_elicitation.html#price-distributions",
    "href": "case-studies/pricedist_elicitation/price_distribution_elicitation.html#price-distributions",
    "title": "Price Distribution Elicitation from an LLM",
    "section": "Price Distributions",
    "text": "Price Distributions\nThe elicited price range tends to overestimate actual prices more often than it underestimates them. This is not particularly surprising if we assume that the distribution of market prices for a given product is right-skewed, as is the case with a log-normal distribution.\nTo reason more clearly about how these price estimates come about, we adopt a stylized model of the generative process:\nFor each product type \\(i\\), the LLM is exposed during training to a sample of \\(k_i\\) observed prices.\nIt returns:\n\nthe minimum of this sample as the lower bound,\nthe maximum as the upper bound, and\na typical price that likely corresponds to the median or mode of the sample (theoretically, the mode would be most defensible as a ‚Äútypical‚Äù price).\n\nOf course, this model is a deliberate oversimplification. In practice, most products are not represented verbatim in the training data. Instead, the LLM likely draws on similar products and generalizes across categories. But even so, this toy model helps clarify what kind of information the LLM is plausibly encoding and returning ‚Äî namely, some noisy abstraction over a finite sample of real-world prices with unknown size.\n\nCodeproduct_prices %&gt;% summarize(\n  perc_list_price_lower = mean(price_usd &lt; lower),\n  perc_list_price_higher = mean(price_usd &gt; upper) \n\n\nSimple Log-Normal Model\n\nThe goal is to use the LLM-generated estimates to infer a plausible price distribution for each product.\nIdeally, this would involve modeling the generative process explicitly‚Äîtreating the elicited range and typical price as functions of latent price distributions, and marginalizing over unknown parameters such as the sample size and distributional shape.\nIn practice, however, we take a simpler approach: we calibrate the parameters of a log-normal distribution such that the observed proportions of actual prices falling below or above the elicited range align with the expected coverage probabilities. Given our above-stated assumptions, we can reverse-engineer the values of the log-normal parameters \\(\\mu_i\\), \\(\\sigma_i\\) for each product \\(i\\), using the relationship below. Solving these equations gives estimates of \\(\\mu\\) and \\(\\sigma\\) for each product.\n\n\\[ log(\\text{lower}) = \\mu + z_{0.3} \\cdot \\sigma \\] \\[ log(\\text{upper}) = \\mu + z_{0.8} \\cdot \\sigma \\]\n\nCode# Define z-scores for assumed quantile boundaries of the elicited range\nz_lower &lt;- qnorm(0.3)\nz_upper &lt;- qnorm(0.8)\n\n# Calibrate log-normal parameters assuming lower and upper are 30% and 80% quantiles\nproduct_prices &lt;- \n  py$product_prices_pd %&gt;%\n  filter(!is.na(lower) & !is.na(upper) & !is.na(price_usd)) %&gt;%\n  mutate(\n    p1_log_sigma = (log(upper) - log(lower)) / (z_upper - z_lower),\n    p1_log_mu = log(lower) - p1_log_sigma * z_lower\n  )\n\n\nWe then validate this calibration by checking how well the empirical price data agrees with the implied distribution. Specifically, we:\n\nUse the calibrated \\(\\mu\\) and \\(\\sigma\\) to compute a range of quantiles.\nFor each theoretical quantile, we calculate the proportion of real prices that fall under it.\nWe compare these empirical proportions to the theoretical coverage probabilities.\n\nIf the calibration is good, the empirical and theoretical values should match closely. If not, it suggests that either:\n\nThe log-normal assumption is inadequate\nThe quantile interpretation of the LLM‚Äôs bounds is incorrect\nOr there are systematic biases (e.g., skewed sampling, outliers, etc.)\n\n\nCode# Define theoretical cumulative probabilities to test calibration\ncalibration_probs &lt;- c(0.01, seq(0.05, 0.95, 0.05), 0.99)\n\n# For each quantile level, compute predicted price threshold and empirical coverage\nempirical_cdfs &lt;- sapply(calibration_probs, function(p) {\n  predicted_threshold &lt;- with(product_prices, exp(p1_log_mu + p1_log_sigma * qnorm(p)))\n  mean(product_prices$price_usd &lt; predicted_threshold)\n})\n\n# Assemble calibration data frame\ncalibration_df &lt;- data.frame(\n  theoretical_cdf = calibration_probs,\n  empirical_cdf = empirical_cdfs\n)\n\n# Plot calibration curve\nggplot(calibration_df, aes(x = theoretical_cdf, y = empirical_cdf)) +\n  geom_point() + geom_line() +\n  geom_abline(slope = 1, intercept = 0, linetype = \"dotted\", color = \"red\") +\n  labs(\n    x = \"Target quantile (log-normal model)\",\n    y = \"Observed proportion below quantile\",\n    title = \"Calibration Plot: Log-normal Model vs. Observed Prices\"\n  ) +\n  theme_bw()\n\n\nIt seems that the probability estimates are well-calibrated. Each theoretical quantile corresponds to an approximately equal empirical proportion.\nShifted Log-Normal Model\n\nWhile the simple log-normal model appears to do a decent job of accounting for individual product price distributions, the log-normal has support on the entire real line, which means that all positive prices are possible, even if they are not very likely.\nIn reality, this may lead to an excessive amount of probability mass being allocated to unrealistically small prices.\nAs a solution, it may make sense to model individual product prices as shifted log-normals.\n\n\n\\[ log(\\text{lower} - delta) = \\mu + z_{0.3} \\cdot \\sigma \\] \\[ log(\\text{mode} - delta) = \\mu -\\sigma^2 \\] \\[ log(\\text{median} - delta) = \\mu + z_{0.5} \\cdot \\sigma \\] \\[ log(\\text{upper} - delta) = \\mu + z_{0.8} \\cdot \\sigma \\]\n\nCode# Define z-scores for assumed quantile boundaries of the elicited range\nz_lower &lt;- qnorm(0.3)\nz_upper &lt;- qnorm(0.8)\n\n# Calibrate log-normal parameters assuming lower and upper are 30% and 80% quantiles\nproduct_prices &lt;- \n  py$product_prices_pd %&gt;%\n  filter(!is.na(lower) & !is.na(upper) & !is.na(price_usd)) %&gt;%\n  mutate(\n    p2_delta = (log(upper) - log(lower)) / (z_upper - z_lower),\n    p2_log_sigma = (log(upper) - log(lower)) / (z_upper - z_lower),\n    p2_log_mu = log(lower) - p1_log_sigma * z_lower\n  )\n\n\nWe then validate this calibration by checking how well the empirical price data agrees with the implied distribution. Specifically, we:\n\nUse the calibrated \\(\\mu\\) and \\(\\sigma\\) to compute a range of quantiles.\nFor each theoretical quantile, we calculate the proportion of real prices that fall under it.\nWe compare these empirical proportions to the theoretical coverage probabilities.\n\nIf the calibration is good, the empirical and theoretical values should match closely. If not, it suggests that either:\n\nThe log-normal assumption is inadequate\nThe quantile interpretation of the LLM‚Äôs bounds is incorrect\nOr there are systematic biases (e.g., skewed sampling, outliers, etc.)\n\n\nCode# Define theoretical cumulative probabilities to test calibration\ncalibration_probs &lt;- c(0.01, seq(0.05, 0.95, 0.05), 0.99)\n\n# For each quantile level, compute predicted price threshold and empirical coverage\nempirical_cdfs &lt;- sapply(calibration_probs, function(p) {\n  predicted_threshold &lt;- with(product_prices, exp(p1_log_mu + p1_log_sigma * qnorm(p)))\n  mean(product_prices$price_usd &lt; predicted_threshold)\n})\n\n# Assemble calibration data frame\ncalibration_df &lt;- data.frame(\n  theoretical_cdf = calibration_probs,\n  empirical_cdf = empirical_cdfs\n)\n\n# Plot calibration curve\nggplot(calibration_df, aes(x = theoretical_cdf, y = empirical_cdf)) +\n  geom_point() +\n  geom_abline(slope = 1, intercept = 0, linetype = \"dotted\", color = \"red\") +\n  labs(\n    x = \"Target quantile (log-normal model)\",\n    y = \"Observed proportion below quantile\",\n    title = \"Calibration Plot: Log-normal Model vs. Observed Prices\"\n  ) +\n  theme_bw()\n\n\nIt seems that the probability estimates are well-calibrated. Each theoretical quantile corresponds to an approximately equal empirical proportion."
  },
  {
    "objectID": "case-studies/pricedist_elicitation/price_distribution_elicitation.html#repository",
    "href": "case-studies/pricedist_elicitation/price_distribution_elicitation.html#repository",
    "title": "Price Distribution Elicitation from an LLM",
    "section": "Repository",
    "text": "Repository\nAll data and source code are available here: üëâ https://github.com/plogacev/case_studies/tree/main/price_distribution_elicitation"
  },
  {
    "objectID": "case-studies/time_series_analysis_1/ts_1_synthetic_data.html",
    "href": "case-studies/time_series_analysis_1/ts_1_synthetic_data.html",
    "title": "Generating Synthetic Order Volume",
    "section": "",
    "text": "Understanding real-world sales patterns often requires modeling specific aspects of the data which can be obscured by factors that are of no theoretical interest. For instance, the effects of price sensitivity may be obscured by seasonal patterns, seasonal patterns may be partially obscured by one-off events, effects of consumer confidence, or by long-term trends, among others. By constructing a synthetic dataset with well-defined components, we can test whether a model can effectively recover these hidden relationshop.\nThis notebook creates synthetic sales data for an online store and explains the logic behind its generation. The goal is to simulate a realistic sales time series incorporating latent seasonality (both weekly and yearly), latent growth over time, as well as influences of unmodeled factors via a random walk. The generated data can be used for testing time series models that aim to uncover some of these latent structures.\nKey aspects of our data:\n\n\nLatent Growth. Sales increase gradually, accelerating after a certain point, and then saturate.\n\nYearly Seasonality. Sales vary throughout the year, with a peak during certain periods (e.g., summer or holiday seasons).\n\nWeekly Seasonality. A periodic pattern emerges within each week (e.g., higher sales on weekends).\n\nRandom Walk Noise. Unmodeled variations and external shocks are captured through a random walk process, ensuring realistic fluctuations.\n\n\nCodeimport polars as pl\nimport numpy as np\nimport pandas as pd\nfrom plotnine import ggplot, aes, geom_line, labs, theme_minimal, theme_bw, scale_x_continuous, scale_x_discrete, scale_x_datetime\n\n\n\nCode# Creates a simple plot using plotnine\ndef plot_function(x, y, title, xlab, ylab):\n    # Convert x to numpy array\n    x = np.array(x)\n    \n    # Check if y is a callable function\n    if callable(y):\n        # If y is a function, apply it to x and create a DataFrame\n        df = pd.DataFrame({\"x\": x, \"y\": y(x)})\n    else:\n        # If y is not a function, create a DataFrame directly\n        df = pd.DataFrame({\"x\": x, \"y\": y})        \n\n    # Create the plot using ggplot\n    plot = (ggplot(df, aes(x=\"x\", y=\"y\")) + geom_line() + labs(title=title, x=xlab, y=ylab) + theme_bw())\n    \n    return plot\n\n\n\nIn the initial steps, we will create a number of independent components that contribute to the price independently. In principle, the way to interpret them is ‚Äòthis is what sales would look like over the time frame considered is all else remained equal‚Äô.\n\nA daily time series is generated starting from June 1, 2021, up to the present day. This ensures sufficient data points to analyze trends and seasonality.\n\nCode# set the random seed\nnp.random.seed(42)\n\n# define date range\nstart_date = \"2021-06-01\"\nend_date = pd.Timestamp.today().strftime(\"%Y-%m-%d\")\ndate_range = pd.date_range(start=start_date, end=end_date, freq=\"D\")\n\n# generate time index\ndays_since_start = np.arange(len(date_range))\n\n\n\nLong-term sales growth is modeled as a modified logistic function, representing an initial slow growth phase, followed by acceleration, and eventual saturation. This function ensures that sales begin near zero, increase slowly at first, then accelerate before stabilizing.\n\\[ f(t)  = L \\cdot (1 + exp(-k \\cdot (t - t_0)))^{(-1/v)} \\]\nParameters: - L. Upper bound on sales (saturation level). - k. Growth rate. - x_0. Inflection point (where acceleration peaks). - v. Asymmetry parameter (v &lt; 1 slower growth to the right of x0, 0 &lt; v &lt; 1 slower growth to the left of x0)\n\nCode# Logistic function parameters\nL = 1      # Upper saturation limit\nk = 0.0125 # Growth rate\nt0 = 1100  # Inflection point\nv = 5      # Asymmetry parameter (v &gt; 1 shifts right, 0 &lt; v &lt; 1 shifts left)\n\n# Compute the logistic growth curve\ngrowth_fn = lambda t: L * (1 + v * np.exp(-k * (t - t0)))**(-1/v)\ngrowth = growth_fn(days_since_start)\n\np = plot_function(x=date_range, y=growth, title=\"Logistic Growth Over Time\", xlab=\"Days Since Start\", ylab=\"Sales Factor\")\n_ = p.draw(show=True)\n\n\n\nYearly seasonality is introduced using a scaled cosine transformations of the day of the year. This captures periodic effects such as, in this case, increased summer sales. This function is scaled to oscillate between 0.8 and 1.2 over the course of a year, which, when multiplied with the average sales function futher down will decrease winter sales by up to \\(20\\%\\), and increase summer sales by up to \\(20\\%\\).\n\nCode# Yearly seasonality\nyearly_seasonality_fn = lambda day: 1 + 0.2 * np.cos(2 * np.pi * day / 365.25 - np.pi)\nyearly_seasonality = yearly_seasonality_fn(date_range.day_of_year)\n\np = plot_function(x=range(0,366), y=yearly_seasonality_fn, title=\"Yearly Seasonality\", xlab=\"Day of the Year\", ylab=\"Sales Factor\")\n_ = p.draw(show=True)\n\n\n\nWeekly seasonality is also modeled a scaled cosine transformations of the day of the week. In this case, we model a drop in sales, primarily on Thursday-Friday. This function is scaled to oscillate between 0.9 and 1.1 over the course of the week, which, when multiplied with the average sales function futher will change sales by \\(\\pm 10\\%\\).\n\nCode# Weekly seasonality\nweekly_seasonality_fn = lambda day: 1 + 0.1 * np.cos(2 * np.pi * day / 7)\nweekly_seasonality = weekly_seasonality_fn(date_range.weekday)\n\np = plot_function(x=range(0,7), y=weekly_seasonality_fn, title=\"Weekly Seasonality\", xlab=\"Day of the Week\", ylab=\"Sales Factor\") \n_ = p.draw(show=True)\n\n\n\n\nCodesales = np.array(growth) * np.array( yearly_seasonality ) * np.array( weekly_seasonality )\nbreaks = [pd.Timestamp(d) for d in [\"2017-01-01\", \"2019-01-01\", \"2021-01-01\", \"2023-01-01\", \"2025-01-01\"]]\n\np = plot_function(x=date_range, y=sales, title=\"Growth + Seasonality\", xlab=\"Date\", ylab=\"Sales Factor\") + scale_x_datetime(breaks = breaks)\n_ = p.draw(show=True)\n\n\n\nA random walk is used to simulate external influences and unpredictable variations. This component accounts for factors not explicitly modeled, such as promotions, economic shifts, or changes in popularity, or influences of competitors. The random walk is centered (mean zero) to ensure it does not systematically bias the trend. This ensures that the long-term sales trajectory remains driven by the logistic growth component rather than arbitrary drift. This does not amount to any sort of assumption about the data-generating process in a more realistic scenario. This is done strictly to maintain interpretability in the bringing together of the different parts of the synthetic demand.\n\nCodenp.random.seed(441)\n\nrandom_walk = np.cumsum(np.random.normal(scale=.015, size=len(date_range)))\ncentered_random_walk = random_walk - np.mean(random_walk)\n\np = plot_function(x=date_range, y = centered_random_walk, title=\"Random Walk Component\", xlab=\"Date\", ylab=\"Latent Sales\") + scale_x_datetime(breaks = breaks)\n_ = p.draw(show=True)\n\n\nLet‚Äôs visualize the growth together with the random walk for future reference, because they will be estimated as one component in the model in the next notebook.\n\nCodegrowth_plus_rw = np.exp( np.log(growth) + centered_random_walk)\np = plot_function(x=date_range, y = growth_plus_rw, title=\"Growth + Random Walk Component\", xlab=\"Date\", ylab=\"Latent Sales\") + scale_x_datetime(breaks = breaks)\n_ = p.draw(show=True)\n\n\nThe random walk is combined with the sales pattern created so far in log-space in order to ensure that the effects are multiplicative. This aligns with real-world sales data, where sales fluctuations are typically proportional rather than absolute. This is also a simple way of preventing sales from dropping below 0.\n\nCodesales_with_random_component = np.exp( np.log(sales) + centered_random_walk)\np = plot_function(x=date_range, y = sales_with_random_component, title=\"Growth + Seasonality + Random Walk\", xlab=\"Date\", ylab=\"Latent Sales\") + scale_x_datetime(breaks = breaks)\n_ = p.draw(show=True)\n\n\n\n\nCodedef sample_log_price_change(n, p, min_delta, max_delta):\n    \"\"\"Sample n values from a mixture of:\n    - 0 with probability p\n    - Uniform(min_delta, max_delta) with probability (1-p)\n    \"\"\"\n    zero_mask = np.random.rand(n) &lt; p  # Boolean mask for zeros\n    delta_log_price_nonzero = np.random.uniform(min_delta, max_delta, n)  # Sample from Uniform(a, b)\n    \n    # Combine: replace values with 0 where zero_mask is True\n    delta_log_price = np.where(zero_mask, 0, delta_log_price_nonzero)\n    return delta_log_price\n\n\n\nCodedelta_log_price = [0.0]*len(date_range)\ndelta_log_price[150] = .1\ndelta_log_price[300] = .1\ndelta_log_price[500] = -.15\ndelta_log_price[750] = .1\ndelta_log_price[1000] = .1\ndelta_log_price[1200] = .05\np = plot_function(x=date_range, y = np.cumsum(delta_log_price), title=\"Difference in log price to baseline\", xlab=\"Date\", ylab=\"Œî log(price)\") + scale_x_datetime(breaks = breaks)\n_ = p.draw(show=True)\n\n\n\nCodeprice_base = 20\nlog_price = np.log(price_base) + np.cumsum(delta_log_price)\np = plot_function(x=date_range, y = np.exp( log_price ), title=\"Product Price\", xlab=\"Date\", ylab=\"Price\") + scale_x_datetime(breaks = breaks)\n_ = p.draw(show=True)\n\n\n\nCodeelasticity = -1.4\nsales_with_price_effect = np.exp( np.log(sales_with_random_component) + elasticity * (log_price - np.mean(log_price)) )\np = plot_function(x=date_range, y = sales_with_price_effect, title=\"\", xlab=\"Day of the Week\", ylab=\"Latent Sales\") + scale_x_datetime(breaks = breaks)\n_ = p.draw(show=True)\n\n\n\nAt this point, we scale the expected sales to a more realistic range for actual sales.\n\nCodemax_sales = 200 # scale the sales to a more realistic range\nscale_factor = max_sales / sales_with_price_effect.max()\nsales_scaled = scale_factor * sales_with_price_effect \n\np = plot_function(x=date_range, y = sales_scaled, title=\"Scaled Latent Sales\", xlab=\"Date\", ylab=\"Latent Sales\") + scale_x_datetime(breaks = breaks)\n_ = p.draw(show=True)\n\n\n\nWhat we constructed until now are the expected sales \\(\\lambda\\) for each day. We realize them for each day \\(i\\) by drawing them from a Poisson distribution with parameter \\(\\lambda_i\\). This approach ensures that while the underlying sales structure is generated smoothly, the final dataset exhibits realistic integer sales values with appropriate stochastic variation.\n\nCodesales_realized = np.random.poisson(lam=sales_scaled)\np = plot_function(x=date_range, y = sales_realized, title=\"Realized Sales\", xlab=\"Date\", ylab=\"Latent Sales\") + scale_x_datetime(breaks = breaks)\n_ = p.draw(show=True)\n\n\n\nHere, we check that the composition of the sales time series is as expected. We‚Äôll estimate regression coefficients for all the components. All should be 1, with the exception of the effect of price, which should equal the specified elastictiy. Please note though, that although all components are additive in log-space, their contributions are not equal, since they are scaled differently.\n\nCodeimport statsmodels.api as sm\n\n# fit poisson glm\nX = pd.DataFrame({\n    'intercept': np.ones(len(date_range)),  # Intercept\n    'centered_random_walk': centered_random_walk,\n    'weekly_seasonality': np.log(weekly_seasonality),\n    'yearly_seasonality': np.log(yearly_seasonality),\n    'growth': np.log(growth),\n    'log_price': log_price\n})\n\ny = sales_realized\npoisson_model = sm.GLM(y, X, family=sm.families.Poisson()).fit()\nsummary = poisson_model.summary()\nprint( summary )\n\n\n\nHaving instantiated the sales time series, we save the latent and realized sales in CSV format.\n\nCodedf = pl.DataFrame({\n    \"date\": date_range.astype(str).tolist(),\n    \"log_price\": log_price,  \n    \"sales_latent\": sales_scaled,\n    \"sales\": sales_realized\n})\ndf.write_csv(\"sales_synthetic.csv\")\n\n\n\nCodeimport pickle\n\nwith open(\"sim_parameters.pkl\", \"wb\") as f:\n    wdays = list(range(0, 7))\n    weekly_seasonality = [weekly_seasonality_fn(wday) for wday in wdays]\n    yearly_seasonality = yearly_seasonality_fn(date_range.day_of_year)\n    sim_parameters = date_range, growth, growth_plus_rw, scale_factor, wdays, weekly_seasonality, yearly_seasonality\n    pickle.dump(sim_parameters, f)"
  },
  {
    "objectID": "case-studies/time_series_analysis_1/ts_1_synthetic_data.html#introduction",
    "href": "case-studies/time_series_analysis_1/ts_1_synthetic_data.html#introduction",
    "title": "Generating Synthetic Order Volume",
    "section": "",
    "text": "Understanding real-world sales patterns often requires modeling specific aspects of the data which can be obscured by factors that are of no theoretical interest. For instance, the effects of price sensitivity may be obscured by seasonal patterns, seasonal patterns may be partially obscured by one-off events, effects of consumer confidence, or by long-term trends, among others. By constructing a synthetic dataset with well-defined components, we can test whether a model can effectively recover these hidden relationshop.\nThis notebook creates synthetic sales data for an online store and explains the logic behind its generation. The goal is to simulate a realistic sales time series incorporating latent seasonality (both weekly and yearly), latent growth over time, as well as influences of unmodeled factors via a random walk. The generated data can be used for testing time series models that aim to uncover some of these latent structures.\nKey aspects of our data:\n\n\nLatent Growth. Sales increase gradually, accelerating after a certain point, and then saturate.\n\nYearly Seasonality. Sales vary throughout the year, with a peak during certain periods (e.g., summer or holiday seasons).\n\nWeekly Seasonality. A periodic pattern emerges within each week (e.g., higher sales on weekends).\n\nRandom Walk Noise. Unmodeled variations and external shocks are captured through a random walk process, ensuring realistic fluctuations."
  },
  {
    "objectID": "case-studies/time_series_analysis_1/ts_1_synthetic_data.html#import-required-libraries-and-define-functions",
    "href": "case-studies/time_series_analysis_1/ts_1_synthetic_data.html#import-required-libraries-and-define-functions",
    "title": "Generating Synthetic Order Volume",
    "section": "",
    "text": "Codeimport polars as pl\nimport numpy as np\nimport pandas as pd\nfrom plotnine import ggplot, aes, geom_line, labs, theme_minimal, theme_bw, scale_x_continuous, scale_x_discrete, scale_x_datetime\n\n\n\nCode# Creates a simple plot using plotnine\ndef plot_function(x, y, title, xlab, ylab):\n    # Convert x to numpy array\n    x = np.array(x)\n    \n    # Check if y is a callable function\n    if callable(y):\n        # If y is a function, apply it to x and create a DataFrame\n        df = pd.DataFrame({\"x\": x, \"y\": y(x)})\n    else:\n        # If y is not a function, create a DataFrame directly\n        df = pd.DataFrame({\"x\": x, \"y\": y})        \n\n    # Create the plot using ggplot\n    plot = (ggplot(df, aes(x=\"x\", y=\"y\")) + geom_line() + labs(title=title, x=xlab, y=ylab) + theme_bw())\n    \n    return plot"
  },
  {
    "objectID": "case-studies/time_series_analysis_1/ts_1_synthetic_data.html#data-generation-process",
    "href": "case-studies/time_series_analysis_1/ts_1_synthetic_data.html#data-generation-process",
    "title": "Generating Synthetic Order Volume",
    "section": "",
    "text": "In the initial steps, we will create a number of independent components that contribute to the price independently. In principle, the way to interpret them is ‚Äòthis is what sales would look like over the time frame considered is all else remained equal‚Äô.\n\nA daily time series is generated starting from June 1, 2021, up to the present day. This ensures sufficient data points to analyze trends and seasonality.\n\nCode# set the random seed\nnp.random.seed(42)\n\n# define date range\nstart_date = \"2021-06-01\"\nend_date = pd.Timestamp.today().strftime(\"%Y-%m-%d\")\ndate_range = pd.date_range(start=start_date, end=end_date, freq=\"D\")\n\n# generate time index\ndays_since_start = np.arange(len(date_range))\n\n\n\nLong-term sales growth is modeled as a modified logistic function, representing an initial slow growth phase, followed by acceleration, and eventual saturation. This function ensures that sales begin near zero, increase slowly at first, then accelerate before stabilizing.\n\\[ f(t)  = L \\cdot (1 + exp(-k \\cdot (t - t_0)))^{(-1/v)} \\]\nParameters: - L. Upper bound on sales (saturation level). - k. Growth rate. - x_0. Inflection point (where acceleration peaks). - v. Asymmetry parameter (v &lt; 1 slower growth to the right of x0, 0 &lt; v &lt; 1 slower growth to the left of x0)\n\nCode# Logistic function parameters\nL = 1      # Upper saturation limit\nk = 0.0125 # Growth rate\nt0 = 1100  # Inflection point\nv = 5      # Asymmetry parameter (v &gt; 1 shifts right, 0 &lt; v &lt; 1 shifts left)\n\n# Compute the logistic growth curve\ngrowth_fn = lambda t: L * (1 + v * np.exp(-k * (t - t0)))**(-1/v)\ngrowth = growth_fn(days_since_start)\n\np = plot_function(x=date_range, y=growth, title=\"Logistic Growth Over Time\", xlab=\"Days Since Start\", ylab=\"Sales Factor\")\n_ = p.draw(show=True)\n\n\n\nYearly seasonality is introduced using a scaled cosine transformations of the day of the year. This captures periodic effects such as, in this case, increased summer sales. This function is scaled to oscillate between 0.8 and 1.2 over the course of a year, which, when multiplied with the average sales function futher down will decrease winter sales by up to \\(20\\%\\), and increase summer sales by up to \\(20\\%\\).\n\nCode# Yearly seasonality\nyearly_seasonality_fn = lambda day: 1 + 0.2 * np.cos(2 * np.pi * day / 365.25 - np.pi)\nyearly_seasonality = yearly_seasonality_fn(date_range.day_of_year)\n\np = plot_function(x=range(0,366), y=yearly_seasonality_fn, title=\"Yearly Seasonality\", xlab=\"Day of the Year\", ylab=\"Sales Factor\")\n_ = p.draw(show=True)\n\n\n\nWeekly seasonality is also modeled a scaled cosine transformations of the day of the week. In this case, we model a drop in sales, primarily on Thursday-Friday. This function is scaled to oscillate between 0.9 and 1.1 over the course of the week, which, when multiplied with the average sales function futher will change sales by \\(\\pm 10\\%\\).\n\nCode# Weekly seasonality\nweekly_seasonality_fn = lambda day: 1 + 0.1 * np.cos(2 * np.pi * day / 7)\nweekly_seasonality = weekly_seasonality_fn(date_range.weekday)\n\np = plot_function(x=range(0,7), y=weekly_seasonality_fn, title=\"Weekly Seasonality\", xlab=\"Day of the Week\", ylab=\"Sales Factor\") \n_ = p.draw(show=True)\n\n\n\n\nCodesales = np.array(growth) * np.array( yearly_seasonality ) * np.array( weekly_seasonality )\nbreaks = [pd.Timestamp(d) for d in [\"2017-01-01\", \"2019-01-01\", \"2021-01-01\", \"2023-01-01\", \"2025-01-01\"]]\n\np = plot_function(x=date_range, y=sales, title=\"Growth + Seasonality\", xlab=\"Date\", ylab=\"Sales Factor\") + scale_x_datetime(breaks = breaks)\n_ = p.draw(show=True)\n\n\n\nA random walk is used to simulate external influences and unpredictable variations. This component accounts for factors not explicitly modeled, such as promotions, economic shifts, or changes in popularity, or influences of competitors. The random walk is centered (mean zero) to ensure it does not systematically bias the trend. This ensures that the long-term sales trajectory remains driven by the logistic growth component rather than arbitrary drift. This does not amount to any sort of assumption about the data-generating process in a more realistic scenario. This is done strictly to maintain interpretability in the bringing together of the different parts of the synthetic demand.\n\nCodenp.random.seed(441)\n\nrandom_walk = np.cumsum(np.random.normal(scale=.015, size=len(date_range)))\ncentered_random_walk = random_walk - np.mean(random_walk)\n\np = plot_function(x=date_range, y = centered_random_walk, title=\"Random Walk Component\", xlab=\"Date\", ylab=\"Latent Sales\") + scale_x_datetime(breaks = breaks)\n_ = p.draw(show=True)\n\n\nLet‚Äôs visualize the growth together with the random walk for future reference, because they will be estimated as one component in the model in the next notebook.\n\nCodegrowth_plus_rw = np.exp( np.log(growth) + centered_random_walk)\np = plot_function(x=date_range, y = growth_plus_rw, title=\"Growth + Random Walk Component\", xlab=\"Date\", ylab=\"Latent Sales\") + scale_x_datetime(breaks = breaks)\n_ = p.draw(show=True)\n\n\nThe random walk is combined with the sales pattern created so far in log-space in order to ensure that the effects are multiplicative. This aligns with real-world sales data, where sales fluctuations are typically proportional rather than absolute. This is also a simple way of preventing sales from dropping below 0.\n\nCodesales_with_random_component = np.exp( np.log(sales) + centered_random_walk)\np = plot_function(x=date_range, y = sales_with_random_component, title=\"Growth + Seasonality + Random Walk\", xlab=\"Date\", ylab=\"Latent Sales\") + scale_x_datetime(breaks = breaks)\n_ = p.draw(show=True)\n\n\n\n\nCodedef sample_log_price_change(n, p, min_delta, max_delta):\n    \"\"\"Sample n values from a mixture of:\n    - 0 with probability p\n    - Uniform(min_delta, max_delta) with probability (1-p)\n    \"\"\"\n    zero_mask = np.random.rand(n) &lt; p  # Boolean mask for zeros\n    delta_log_price_nonzero = np.random.uniform(min_delta, max_delta, n)  # Sample from Uniform(a, b)\n    \n    # Combine: replace values with 0 where zero_mask is True\n    delta_log_price = np.where(zero_mask, 0, delta_log_price_nonzero)\n    return delta_log_price\n\n\n\nCodedelta_log_price = [0.0]*len(date_range)\ndelta_log_price[150] = .1\ndelta_log_price[300] = .1\ndelta_log_price[500] = -.15\ndelta_log_price[750] = .1\ndelta_log_price[1000] = .1\ndelta_log_price[1200] = .05\np = plot_function(x=date_range, y = np.cumsum(delta_log_price), title=\"Difference in log price to baseline\", xlab=\"Date\", ylab=\"Œî log(price)\") + scale_x_datetime(breaks = breaks)\n_ = p.draw(show=True)\n\n\n\nCodeprice_base = 20\nlog_price = np.log(price_base) + np.cumsum(delta_log_price)\np = plot_function(x=date_range, y = np.exp( log_price ), title=\"Product Price\", xlab=\"Date\", ylab=\"Price\") + scale_x_datetime(breaks = breaks)\n_ = p.draw(show=True)\n\n\n\nCodeelasticity = -1.4\nsales_with_price_effect = np.exp( np.log(sales_with_random_component) + elasticity * (log_price - np.mean(log_price)) )\np = plot_function(x=date_range, y = sales_with_price_effect, title=\"\", xlab=\"Day of the Week\", ylab=\"Latent Sales\") + scale_x_datetime(breaks = breaks)\n_ = p.draw(show=True)\n\n\n\nAt this point, we scale the expected sales to a more realistic range for actual sales.\n\nCodemax_sales = 200 # scale the sales to a more realistic range\nscale_factor = max_sales / sales_with_price_effect.max()\nsales_scaled = scale_factor * sales_with_price_effect \n\np = plot_function(x=date_range, y = sales_scaled, title=\"Scaled Latent Sales\", xlab=\"Date\", ylab=\"Latent Sales\") + scale_x_datetime(breaks = breaks)\n_ = p.draw(show=True)"
  },
  {
    "objectID": "case-studies/time_series_analysis_1/ts_1_synthetic_data.html#realized-sales",
    "href": "case-studies/time_series_analysis_1/ts_1_synthetic_data.html#realized-sales",
    "title": "Generating Synthetic Order Volume",
    "section": "",
    "text": "What we constructed until now are the expected sales \\(\\lambda\\) for each day. We realize them for each day \\(i\\) by drawing them from a Poisson distribution with parameter \\(\\lambda_i\\). This approach ensures that while the underlying sales structure is generated smoothly, the final dataset exhibits realistic integer sales values with appropriate stochastic variation.\n\nCodesales_realized = np.random.poisson(lam=sales_scaled)\np = plot_function(x=date_range, y = sales_realized, title=\"Realized Sales\", xlab=\"Date\", ylab=\"Latent Sales\") + scale_x_datetime(breaks = breaks)\n_ = p.draw(show=True)"
  },
  {
    "objectID": "case-studies/time_series_analysis_1/ts_1_synthetic_data.html#sanity-check",
    "href": "case-studies/time_series_analysis_1/ts_1_synthetic_data.html#sanity-check",
    "title": "Generating Synthetic Order Volume",
    "section": "",
    "text": "Here, we check that the composition of the sales time series is as expected. We‚Äôll estimate regression coefficients for all the components. All should be 1, with the exception of the effect of price, which should equal the specified elastictiy. Please note though, that although all components are additive in log-space, their contributions are not equal, since they are scaled differently.\n\nCodeimport statsmodels.api as sm\n\n# fit poisson glm\nX = pd.DataFrame({\n    'intercept': np.ones(len(date_range)),  # Intercept\n    'centered_random_walk': centered_random_walk,\n    'weekly_seasonality': np.log(weekly_seasonality),\n    'yearly_seasonality': np.log(yearly_seasonality),\n    'growth': np.log(growth),\n    'log_price': log_price\n})\n\ny = sales_realized\npoisson_model = sm.GLM(y, X, family=sm.families.Poisson()).fit()\nsummary = poisson_model.summary()\nprint( summary )"
  },
  {
    "objectID": "case-studies/time_series_analysis_1/ts_1_synthetic_data.html#save-sales",
    "href": "case-studies/time_series_analysis_1/ts_1_synthetic_data.html#save-sales",
    "title": "Generating Synthetic Order Volume",
    "section": "",
    "text": "Having instantiated the sales time series, we save the latent and realized sales in CSV format.\n\nCodedf = pl.DataFrame({\n    \"date\": date_range.astype(str).tolist(),\n    \"log_price\": log_price,  \n    \"sales_latent\": sales_scaled,\n    \"sales\": sales_realized\n})\ndf.write_csv(\"sales_synthetic.csv\")\n\n\n\nCodeimport pickle\n\nwith open(\"sim_parameters.pkl\", \"wb\") as f:\n    wdays = list(range(0, 7))\n    weekly_seasonality = [weekly_seasonality_fn(wday) for wday in wdays]\n    yearly_seasonality = yearly_seasonality_fn(date_range.day_of_year)\n    sim_parameters = date_range, growth, growth_plus_rw, scale_factor, wdays, weekly_seasonality, yearly_seasonality\n    pickle.dump(sim_parameters, f)"
  },
  {
    "objectID": "case-studies/pricedist_changepoints/greedy_cp_selection.html",
    "href": "case-studies/pricedist_changepoints/greedy_cp_selection.html",
    "title": "Bayesian Changepoint Detection on Price Histograms",
    "section": "",
    "text": "Codelibrary(Rcpp)\nsource(\"./source/data_generation.r\")\nsource(\"./source/greedy_cp_selection.r\")\nRcpp::sourceCpp(\"./source/greedy_cp_selection.cpp\")"
  },
  {
    "objectID": "case-studies/pricedist_changepoints/greedy_cp_selection.html#summary",
    "href": "case-studies/pricedist_changepoints/greedy_cp_selection.html#summary",
    "title": "Bayesian Changepoint Detection on Price Histograms",
    "section": "Summary",
    "text": "Summary\nThis notebook demonstrates a Bayesian changepoint detection algorithm for histogram-valued time series. It is based on a greedy search tailored for transactional price data with varying pricing regimes.\n\nEach day‚Äôs price distribution is modeled via discretized histograms.\nThe changepoint configuration is selected via greedy MAP estimation.\nIn the future, parameters may be obtained by means of sampling instead of greedy search. (For instance, using RcppSMC or a custom sampler, e.g., MH)."
  },
  {
    "objectID": "case-studies/pricedist_changepoints/greedy_cp_selection.html#modeling-approach",
    "href": "case-studies/pricedist_changepoints/greedy_cp_selection.html#modeling-approach",
    "title": "Bayesian Changepoint Detection on Price Histograms",
    "section": "Modeling Approach",
    "text": "Modeling Approach\nWe model the price distribution of a product as a histogram over discrete price points, because in B2C scenarios, only a few specifc prices are available at any given time. Some customer groups may be offered a discount, while most purchase at the list price. Modeling daily means or medians may lose the multimodal structure that matters most for detecting pricing regimes. Histogram-valued preserve the full distributional shape, making it possible to detect subtle regime shifts such as list price changes or availability of discounts. The method is useful for detecting changes in pricing regimes, such as list price changes, promotions, or changes in the availability of discounts.\nEach day is represented by a histogram over discrete price points. The time series is transformed into a histogram-valued sequence, represented as a matrix of counts. We assume that pricing follows distinct pricing regimes, each associated with a different price distribution. Changes in pricing regime occur when the pricing of a product undergoes a meaningful change ‚Äî such as list price changes, promotions, or changes in the availability of discounts. Changepoints are defined as time indices where the underlying price distribution regime changes, leading to a change in relative frequency of price points.\nData Format\nThe synthetic data used in this notebook simulates 800 days of pricing activity, segmented into 10 regimes characterized by different list prices, discounts, and discount availability\n\nCodeset.seed(123)\n\nsegments &lt;- data.frame(\n             start_day = c(1,  90, 250, 300, 400, 500, 550, 600, 700, 750),\n            list_price = c(5,    6,   6,   6,   6, 5.5, 5.5, 5.5, 7, 7),\n        discount_price = c(4,    4,   5,   5,   5, 4.5, 4.5, 4.5, 2, 2),\n   discount_percentage = c(.25, .1,  .1, .25,  .5, .01, .03, .15,.3, .4)\n)\n\nn_days &lt;- 800\nlambda_qty &lt;- 20\ndf &lt;- generate_transaction_prices(segments, n_days, lambda_qty, seed = 123)\nhist &lt;- compute_price_histogram(df)\nactual_changepoints &lt;- segments$start_day[-1]-0.5\n\nknitr::kable(segments, caption = \"Price Regimes Overview\")\n\n\nThe plot below shows the synthetic data set, with the actual changepoints marked with dashed vertical lines. This is the dataset we‚Äôll use to test the segmentation algorithm.\n\nCodep_price &lt;- hist$df %&gt;% rename(quantity = qty) %&gt;%  filter(quantity &gt; 0) %&gt;%\n    ggplot(aes(day, price)) + geom_point(aes(size=quantity, alpha = quantity)) + \n    geom_vline( data = data.frame(x = actual_changepoints, changepoint = \"actual\"),\n                aes(xintercept = x, color = changepoint), linetype = \"dashed\") + theme_bw() +\n    theme(legend.position = \"top\") \n\nprint(p_price)\n\n\nModel Structure\nSegmentation\nWe define a segmentation of a histogram-valued time series \\(h\\) of length \\(T\\) as a vector \\(z \\in \\{0, 1\\}^{T-1}\\), where \\(z_t=1\\) indicates a changepoint between indices \\(t\\) and \\(t+1\\), and \\(0\\) indicates its absence. From the index vector \\(z\\), we can derive a segmentation in the form of \\(\\mathcal{S}(z)\\), i.e., a set of all segment intervals \\([t_1, t_2]\\).\nThe model is intended to select a segmentation \\(z\\) that best balances (i) goodness-of-fit (how well the empirical histograms are explained within segments) with (ii) model simplicity (how many changepoints are included). The balance is governed by the regularizing parameter \\(\\lambda\\), which is selected by optimizing the posterior likelihood over it.\nThe posterior over a changepoint configurations \\(z\\) is assumed to be:\n\\[\np(z \\mid \\mathbf{n}, \\lambda) \\propto p(\\mathbf{n} \\mid z) \\cdot p(z) \\cdot p(\\lambda)\n\\]\nLikelihood\nThe data likelihood is computed as the product of likelihoods of the segments defined by the segmentation \\(\\mathcal{S}(z)\\), where \\(\\mathbf{n}_{[t_1, t_2]}\\) stands for the histogram over the interval \\([t_1, t_2]\\).\n\\[\np(\\mathbf{n} \\mid z) = \\prod_{(t_1, t_2) \\in \\mathcal{S}(z)} p(\\mathbf{n}_{[t_1, t_2]})\\text{,}\n\\]\n\nThe likelihood of a segment \\(\\mathbf{n}_{[t_1, t_2]}\\) is set to it (regularized) maximum likelihood estimate ‚Ä¶ (explain more) ‚Ä¶, where \\(\\hat{p}_i\\) is the (regularized) maximum likelihood estimate of the relative frequencies of the different price points. This corresponds to using the maximum likelihood estimate of the multinomial probabilities within each segment. Although technically this is not fully Bayesian (since we‚Äôre not integrating over latent parameters), it can be viewed as an empirical Bayes approximation.\n\\[\np(\\mathbf{n}_{[t_1, t_2]}) = \\sum_{i=1}^{K} (\\hat{p}_i)^{n_i}\n\\]\nIn computing \\(\\hat{p}_i\\), we smooth each bin count with a small constant \\(\\epsilon\\) to prevent division by zero.\n\\[\n\\hat{p}_i = \\frac{n_i + \\epsilon}{\\sum_j (n_j + \\epsilon)}\n\\]\nPrior\nWe place a Bernoulli prior on each potential changepoint, to penalize excessive complexity: small \\(\\lambda\\) values encourage fewer changepoints, favoring parsimony. In consequence, the model selects the segmentation \\(z\\) that best balances goodness-of-fit (how well the empirical histograms are explained within segments) with model simplicity (how many changepoints are included). The balance is governed by \\(\\lambda\\).\n\\[\nz_t \\sim \\text{Bernoulli}(\\lambda)\n\\]"
  },
  {
    "objectID": "case-studies/pricedist_changepoints/greedy_cp_selection.html#estimation",
    "href": "case-studies/pricedist_changepoints/greedy_cp_selection.html#estimation",
    "title": "Bayesian Changepoint Detection on Price Histograms",
    "section": "Estimation",
    "text": "Estimation\nTo estimate the changepoint configuration \\(z\\), we use a greedy forward search:\n\nStart with no changepoints.\nIteratively add the changepoint that most increases the penalized posterior.\nStop when no further improvement is possible.\n\nThis process is repeated for different values of \\(\\lambda\\), and the \\(\\lambda\\) value that maximizes the resulting posterior is selected using one-dimensional optimization (optimize() in R)."
  },
  {
    "objectID": "case-studies/pricedist_changepoints/greedy_cp_selection.html#implementation",
    "href": "case-studies/pricedist_changepoints/greedy_cp_selection.html#implementation",
    "title": "Bayesian Changepoint Detection on Price Histograms",
    "section": "Implementation",
    "text": "Implementation\nThis project implements a greedy changepoint detection algorithm for time series of price histograms. The codebase consists of three components:\n\nA data generator that simulates daily transaction-level price data under piecewise constant pricing regimes (generate_transaction_prices()).\nA transformation step that maps transactional data into a histogram matrix (compute_price_histogram()).\nA C++ backend that performs fast log-likelihood evaluation and greedy changepoint selection via Rcpp."
  },
  {
    "objectID": "case-studies/pricedist_changepoints/greedy_cp_selection.html#changepoint-detection-results",
    "href": "case-studies/pricedist_changepoints/greedy_cp_selection.html#changepoint-detection-results",
    "title": "Bayesian Changepoint Detection on Price Histograms",
    "section": "Changepoint Detection Results",
    "text": "Changepoint Detection Results\nIn the present example, the algorithm detects the same number changepoints, and the estimates ones, are largely fairly close to the simulated ones. A formal evaluation is pending. It stands to reason that the accuracy of changepoint identification will depend on the similarity between adjacent segments, as well as segment length.\n\nCodeopt_res &lt;- locate_optimal_changepoints( hist$histogram, max_lambda = 0.3 )\ndetected_changepoints &lt;- which(opt_res$changepoints) - 0.5\n\nknitr::kable(t(actual_changepoints), caption = \"Original Changepoints\")\nknitr::kable(t(detected_changepoints), caption = \"Detected Changepoints\")\n\n\nThe plot below illustrates the results vis a vis the simulation assumptions.\n\nCodep_price +\n      geom_vline( data = data.frame(x = detected_changepoints, changepoint = \"detected\"),\n                  aes(xintercept = x, color = changepoint), linetype = \"dashed\") +\n      scale_color_manual(name = \"changepoint\", values = c(\"detected\" = \"blue\", \"actual\" = \"red\"),\n                        guide = guide_legend(override.aes = list( linetype = \"solid\", size = 1))\n                        )"
  },
  {
    "objectID": "case-studies/pricedist_changepoints/greedy_cp_selection.html#limitations",
    "href": "case-studies/pricedist_changepoints/greedy_cp_selection.html#limitations",
    "title": "Bayesian Changepoint Detection on Price Histograms",
    "section": "Limitations",
    "text": "Limitations\n\nThe algorithm is designed to detect even small changes in average price if they are sufficiently frequent.\nIn the present version, it cannot deal with days with no sales, and may possibly identify them as a new pricing regime, thus zero-sales days have to be excluded before running it.\nThe algorithm is not designed to deal with overlapping price regimes."
  },
  {
    "objectID": "case-studies/pricedist_changepoints/greedy_cp_selection.html#repository",
    "href": "case-studies/pricedist_changepoints/greedy_cp_selection.html#repository",
    "title": "Bayesian Changepoint Detection on Price Histograms",
    "section": "Repository",
    "text": "Repository\nAll source code is available here:\nüëâ https://github.com/plogacev/case_studies/tree/main/pricedist_changepoints\n\ndouble log1m_exp(double x)\n{\nif (x &gt;= 0.0) stop(\"log1m_exp is undefined for x &gt;= 0\");\n...\n}"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Pavel Logaƒçev",
    "section": "",
    "text": "I‚Äôm a freelance data scientist specializing in statistical modeling for business decisions ‚Äî particularly in settings where data is messy, experimentation is limited, and off-the-shelf solutions fall short.\nMy work spans demand modeling, price elasticity estimation, forecasting, and causal inference. I build custom models using Bayesian methods, designed around the structure of your specific problem.\nI‚Äôve worked across B2C and B2B contexts, collaborating closely with stakeholders to understand how business processes actually work ‚Äî so models reflect what matters and make the most of available data.\nLearn more ‚Üí\n\n    \n    \n  \n\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "case-studies/time_series_analysis_1/index.html",
    "href": "case-studies/time_series_analysis_1/index.html",
    "title": "Time Series Elasticity Analysis",
    "section": "",
    "text": "Sales data can be somewhat chaotic ‚Äî fluctuating strongly due to market trends, changes in advertising strategy, seasonal cycles, and external shocks. Many analytical methods struggle to disentangle meaningful patterns from noise. The present project explores the potential of a Bayesian time series model to break down variance into distinct components.\nWe put it to the test using a synthetic sales dataset designed with well-defined patterns: long-term growth, yearly seasonality, weekly variation, and price sensitivity. The model performed remarkably well, accurately identifying key trends while also quantifying uncertainty. The results highlight the power of Bayesian inference in revealing the drivers of sales dynamics, offering businesses a clearer, data-driven perspective and enabling decision-making that fully accounts for the uncertainty in the estimates."
  },
  {
    "objectID": "case-studies/time_series_analysis_1/index.html#summary",
    "href": "case-studies/time_series_analysis_1/index.html#summary",
    "title": "Time Series Elasticity Analysis",
    "section": "",
    "text": "Sales data can be somewhat chaotic ‚Äî fluctuating strongly due to market trends, changes in advertising strategy, seasonal cycles, and external shocks. Many analytical methods struggle to disentangle meaningful patterns from noise. The present project explores the potential of a Bayesian time series model to break down variance into distinct components.\nWe put it to the test using a synthetic sales dataset designed with well-defined patterns: long-term growth, yearly seasonality, weekly variation, and price sensitivity. The model performed remarkably well, accurately identifying key trends while also quantifying uncertainty. The results highlight the power of Bayesian inference in revealing the drivers of sales dynamics, offering businesses a clearer, data-driven perspective and enabling decision-making that fully accounts for the uncertainty in the estimates."
  },
  {
    "objectID": "case-studies/time_series_analysis_1/index.html#synthetic-data-generation",
    "href": "case-studies/time_series_analysis_1/index.html#synthetic-data-generation",
    "title": "Time Series Elasticity Analysis",
    "section": "1. Synthetic Data Generation",
    "text": "1. Synthetic Data Generation\nüîó [Notebook]"
  },
  {
    "objectID": "case-studies/time_series_analysis_1/index.html#data-analysis",
    "href": "case-studies/time_series_analysis_1/index.html#data-analysis",
    "title": "Time Series Elasticity Analysis",
    "section": "2. Data Analysis",
    "text": "2. Data Analysis\nüîó[Notebook]"
  },
  {
    "objectID": "case-studies/pricedist_elicitation/index.html",
    "href": "case-studies/pricedist_elicitation/index.html",
    "title": "Price Distribution Elicitation from an LLM",
    "section": "",
    "text": "Demonstrates a two-stage pipeline that uses large language models (LLMs) to elicit reasonable price distributions for consumer products when competitor pricing data is insufficient or historical sales data is sparse. It extracts structured product attributes from free-text titles and descriptions (via GPT-3.5-turbo), then uses GPT-4o to elicit a retail price range (minimum, typical, maximum) based on those attributes and regional context. The elicited price ranges are used to reverse-engineer log-normal distribution parameters, which are validated against actual prices scraped from Mercado Libre. The result is a calibrated, probabilistic estimate of product pricing grounded in LLM-elicited knowledge."
  },
  {
    "objectID": "case-studies/pricedist_elicitation/index.html#summary",
    "href": "case-studies/pricedist_elicitation/index.html#summary",
    "title": "Price Distribution Elicitation from an LLM",
    "section": "",
    "text": "Demonstrates a two-stage pipeline that uses large language models (LLMs) to elicit reasonable price distributions for consumer products when competitor pricing data is insufficient or historical sales data is sparse. It extracts structured product attributes from free-text titles and descriptions (via GPT-3.5-turbo), then uses GPT-4o to elicit a retail price range (minimum, typical, maximum) based on those attributes and regional context. The elicited price ranges are used to reverse-engineer log-normal distribution parameters, which are validated against actual prices scraped from Mercado Libre. The result is a calibrated, probabilistic estimate of product pricing grounded in LLM-elicited knowledge."
  },
  {
    "objectID": "case-studies/pricedist_elicitation/index.html#implementation-with-chatgpt",
    "href": "case-studies/pricedist_elicitation/index.html#implementation-with-chatgpt",
    "title": "Price Distribution Elicitation from an LLM",
    "section": "1. Implementation with ChatGPT",
    "text": "1. Implementation with ChatGPT\nüîó [Notebook]"
  },
  {
    "objectID": "case-studies/pricedist_changepoints/index.html",
    "href": "case-studies/pricedist_changepoints/index.html",
    "title": "Bayesian Changepoint Detection on Price Histograms",
    "section": "",
    "text": "The notebooks in this folder demonstrate multiple implementations of a Bayesian changepoint detection model applied to synthetic sales data, where product prices change over time. The goal is to detect structural changes in pricing regime based on price histograms over time when the pricing of a product undergoes a meaningful change ‚Äî such as list price changes, promotions, or changes in the availability of discounts.\nWe assume that the distribution of prices for a product on any given day depends on the underlying price regime, which changes infrequently. While the price regime (list price and available discounts) remains stable for extended periods, the daily average price can fluctuate substantially due to varying proportions of discounted sales. Therefore, using the average price within a stable regime as a predictor is likely more informative than using volatile daily average prices.\nThe model determines pricing regimes based on daily price histograms (i.e., distributions of sold units across different price points). The first implementation, written in Rcpp, uses discrete parameters. The parameter estimation is carried out using a genetic algorithm implementation in R. The second implementation marginalizes out the discrete segmentation parameters and estimates probabilities of specific changepoint locations. Both are designed to serve as a modular component within larger hierarchical models for retail analytics."
  },
  {
    "objectID": "case-studies/pricedist_changepoints/index.html#summary",
    "href": "case-studies/pricedist_changepoints/index.html#summary",
    "title": "Bayesian Changepoint Detection on Price Histograms",
    "section": "",
    "text": "The notebooks in this folder demonstrate multiple implementations of a Bayesian changepoint detection model applied to synthetic sales data, where product prices change over time. The goal is to detect structural changes in pricing regime based on price histograms over time when the pricing of a product undergoes a meaningful change ‚Äî such as list price changes, promotions, or changes in the availability of discounts.\nWe assume that the distribution of prices for a product on any given day depends on the underlying price regime, which changes infrequently. While the price regime (list price and available discounts) remains stable for extended periods, the daily average price can fluctuate substantially due to varying proportions of discounted sales. Therefore, using the average price within a stable regime as a predictor is likely more informative than using volatile daily average prices.\nThe model determines pricing regimes based on daily price histograms (i.e., distributions of sold units across different price points). The first implementation, written in Rcpp, uses discrete parameters. The parameter estimation is carried out using a genetic algorithm implementation in R. The second implementation marginalizes out the discrete segmentation parameters and estimates probabilities of specific changepoint locations. Both are designed to serve as a modular component within larger hierarchical models for retail analytics."
  },
  {
    "objectID": "case-studies/pricedist_changepoints/index.html#theoretical-model-assumptions",
    "href": "case-studies/pricedist_changepoints/index.html#theoretical-model-assumptions",
    "title": "Bayesian Changepoint Detection on Price Histograms",
    "section": "1. Theoretical Model Assumptions",
    "text": "1. Theoretical Model Assumptions\n[Notebook]"
  },
  {
    "objectID": "case-studies/pricedist_changepoints/index.html#greedy-changepoint-selection",
    "href": "case-studies/pricedist_changepoints/index.html#greedy-changepoint-selection",
    "title": "Bayesian Changepoint Detection on Price Histograms",
    "section": "2. Greedy Changepoint Selection",
    "text": "2. Greedy Changepoint Selection\n[Notebook]"
  },
  {
    "objectID": "blog/index.html",
    "href": "blog/index.html",
    "title": "Blog",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\nNo matching items\n\n Back to top"
  },
  {
    "objectID": "about/index.html",
    "href": "about/index.html",
    "title": "Pavel Logaƒçev",
    "section": "",
    "text": "I help companies make better decisions through statistical modeling ‚Äî especially when data is limited, messy, or poorly understood.\nHow I work\n\nI take time to understand your business context before modeling\nI build interpretable models, not black boxes\nI deliver working solutions ‚Äî not just reports\nI communicate clearly with technical and non-technical stakeholders\n\nGet in touch if you‚Äôd like to work together."
  },
  {
    "objectID": "about/index.html#what-i-do",
    "href": "about/index.html#what-i-do",
    "title": "Pavel Logaƒçev",
    "section": "What I Do",
    "text": "What I Do\n\nPrice elasticity estimation and price optimization\nBayesian methods for sparse data\nDemand modeling and forecasting\nCausal inference (A/B tests, observational studies)\nCustom models tailored to your domain and problem"
  },
  {
    "objectID": "about/index.html#background",
    "href": "about/index.html#background",
    "title": "Pavel Logaƒçev",
    "section": "Background",
    "text": "Background\nPhD in Cognitive Science (University of Potsdam). Before freelancing, I worked in academic research and then transitioned to applied data science in industry.\nI‚Äôve worked with retailers, distributors, and restaurant chains on problems involving pricing, demand, and forecasting. Most of my work is done through agencies, but I also take on direct engagements.\nBased in Berlin. I work with clients across Europe and North America.\nI speak English, German, Russian, and a little Turkish."
  },
  {
    "objectID": "case-studies/index.html",
    "href": "case-studies/index.html",
    "title": "Technical Case Studies",
    "section": "",
    "text": "Detailed case studies with full code and methodology. Each project demonstrates practical applications of Bayesian statistics, time series analysis, and machine learning techniques.\n\n\n\n\n\n\n\n\n\n\n\nPrice Distribution Elicitation from an LLM\n\n\nUsing GPT-4 to elicit calibrated price distributions\n\n\n\n\n\n\n\n\n\n\nA Hidden-Markov Model of Stockout Detection\n\n\nBayesian HMM for detecting unobserved inventory outages\n\n\n\n\n\n\n\n\n\n\nBayesian Changepoint Detection on Price Histograms\n\n\nDetecting structural changes in pricing regimes\n\n\n\n\n\n\n\n\n\n\nTime Series Elasticity Analysis\n\n\nBayesian decomposition of sales dynamics with NumPyro\n\n\n\n\n\n\n\nNo matching items\n\n Back to top"
  },
  {
    "objectID": "case-studies/pricedist_changepoints/model_assumptions.html",
    "href": "case-studies/pricedist_changepoints/model_assumptions.html",
    "title": "Bayesian Changepoint Detection on Price Histograms",
    "section": "",
    "text": "[This is work in progress.]\nAll model implementations in this repo are designed to detect change points in pricing time series data. All models are based on a set of assumptions outlined below: In brief, we will assume that the relative frequency of specific prices for a product on a given day depends on the underlying price regime, which is associated with a specific distribution of prices, and changes rather infrequently. The models in this repo are all designed to detect structural changes in pricing regimes based on daily price histograms. Changepoints occur when the pricing of a product undergoes a meaningful change ‚Äî such as list price changes, promotions, or changes in the availability of discounts.\nThis document outlines the general model assumptions and logic. While none of the actual implementations in this repo actually implement all of them, all models work with some of the assumptions outlined here."
  },
  {
    "objectID": "case-studies/pricedist_changepoints/model_assumptions.html#scope",
    "href": "case-studies/pricedist_changepoints/model_assumptions.html#scope",
    "title": "Bayesian Changepoint Detection on Price Histograms",
    "section": "",
    "text": "[This is work in progress.]\nAll model implementations in this repo are designed to detect change points in pricing time series data. All models are based on a set of assumptions outlined below: In brief, we will assume that the relative frequency of specific prices for a product on a given day depends on the underlying price regime, which is associated with a specific distribution of prices, and changes rather infrequently. The models in this repo are all designed to detect structural changes in pricing regimes based on daily price histograms. Changepoints occur when the pricing of a product undergoes a meaningful change ‚Äî such as list price changes, promotions, or changes in the availability of discounts.\nThis document outlines the general model assumptions and logic. While none of the actual implementations in this repo actually implement all of them, all models work with some of the assumptions outlined here."
  },
  {
    "objectID": "case-studies/pricedist_changepoints/model_assumptions.html#theoretical-generative-model",
    "href": "case-studies/pricedist_changepoints/model_assumptions.html#theoretical-generative-model",
    "title": "Bayesian Changepoint Detection on Price Histograms",
    "section": "Theoretical Generative Model",
    "text": "Theoretical Generative Model\nWe model the daily distribution of purchase prices for a product as dependent on the current latent price regime. Within each regime, there is assumed to be one list price, and one or multiple discounts, at which a product can be purchased. Discounts may have different amounts of availability: for instance, only 2% of customers may be eligible for a 10% discount, while only 1% of customers may be eligible for a 20% discount. Price regimes change occasionally. This can be due to changes of the list price, promotions, changes in the magnitude or availability of discounts, or multiple factors.\nWe assume that each day belongs to a specific pricing regime. At end of the day \\(t\\), the regime either changes (with probability \\(\\rho_t\\)) or remains the same (with probability \\(1 - \\rho_t\\)). Each regime is characterized by a distribution over prices. We assume that it follows a multinomial distribution, such that the distribution of prices in pricing regime \\(k\\) follows \\(\\text{Multinomial}(\\tau_k)\\).\nWe encode a segmentation of a price time series of length \\(T\\) using a changepoint indicator vector \\(z\\) of length \\(T-1\\), where \\(z_t=1\\) indicates a changepoint between time \\(t\\) and \\(t+1\\), and \\(z_t=0\\) indicates the absence of a changepoint. As a result, each indicator vector defines a set of \\(K\\) segments, where \\(t_1^{(k)}\\) and \\(t_2^{(k)}\\) are the start and end points of segment \\(k\\): \\[\n  [t_1^{(1)},\\, t_2^{(1)}], [t_1^{(2)},\\, t_2^{(2)}],\\quad \\dots,\\quad [t_1^{(K)},\\, t_2^{(K)}]\n\\] When the pricing regime changes, it needs to change significantly. Thus, we specify a prior \\(\\theta\\) over the difference between two adjacent price regime distributions, where the actual difference may be specified in any number ways, including a difference between means, the Wasserstein distance, etc.\nThe complete posterior distribution for the changepoint detection model combines the likelihood of the observed data with the prior distributions over the changepoint configuration and the regime differences. The posterior is given by:\n\\[\np(z, \\theta, \\tau \\mid y_{1:T})\n\\propto\np(y_{1:T} \\mid z, \\theta, \\tau)\n\\cdot\np(z, \\theta, \\tau)\\text{, where }\n\\]\n\\[\np(z, \\theta, \\tau) =\np(z)\n\\cdot\np(\\theta)\n\\cdot\np(\\tau)\\text{, and where }\n\\]\n\n\n\\(p(y_{1:T} \\mid z, \\theta, \\tau)\\) is the likelihood of the observed data given the changepoint configuration \\(z\\), the changepoint prior \\(\\theta\\), and the regime parameters \\(\\tau\\).\n\n\\(p(z)\\) is the prior probability of the changepoint configuration, modeled as a Bernoulli process with changepoint probability \\(\\rho\\).\n\n\\(p(\\theta)\\) is the prior distribution over the magnitude of the price regime change, specified based on domain knowledge.\n\n\\(p(\\tau)\\) is the prior distribution over the regime parameters \\(\\tau\\), which incorporates the belief about the differences between successive regimes.\n\nIt follows from the structure of the model that likelihood of \\(y_{1:T}\\) is as below.\n\\[\n  p(y_{1:T} \\mid z, \\theta, \\tau) = \\prod_{k=1}^{K} \\prod_{t=t_1^{(k)}}^{t_2^{(k)}} \\text{Multinomial}(y_t \\mid \\tau_k)\n\\]\nThe prior probability \\(p(z)\\) of a changepoint configuration \\(z\\) can be modeled using a Bernoulli process with a changepoint probability \\(\\rho\\). Assuming each \\(z_t\\) is an independent Bernoulli random variable, the prior can be expressed as below, where \\(\\rho\\) is the probability of a changepoint occurring between any two consecutive time points.\n\n\\[\n  z \\sim Bernoulli(\\rho)\n\\]\nWe further incorporate a prior for the differences between successive \\(\\tau_k\\) to capture the belief that changes in pricing regimes are typically abrupt rather than gradual, and that two adjacent pricing regimes need to be substantially different in order to justify positing a regime switch. In other words, it penalizes small differences between any two adjacent pricing regimes. One example specification is as below, where \\(\\delta\\) is any kind of distance function calculating absolute differences between two distributions.\n\\[\n  \\delta(\\tau_k, \\tau_{k+1}) \\sim Normal( \\theta_{\\mu}, \\theta_{\\sigma} )\n\\]"
  },
  {
    "objectID": "case-studies/stockouts/index.html",
    "href": "case-studies/stockouts/index.html",
    "title": "A Hidden-Markov Model of Stockout Detection",
    "section": "",
    "text": "Bayesian Hidden Markov Model for detecting stockouts in sales data with no explicit inventory information. A synthetic dataset is generated with steady latent demand and known stockout periods to demonstrate the principle. Realized sales are treated as true demand realizations except during stockouts, which structurally force sales to zero and hide actual demand. The model treats sales as emissions from two hidden states - regular sales and stockout - with state transitions governed by a Markov process. Inference is performed in Stan using MAP estimation and the Viterbi algorithm to recover the most likely stockout periods. The result shows how probabilistic state-switching can uncover unobserved stockouts that distort sales data."
  },
  {
    "objectID": "case-studies/stockouts/index.html#summary",
    "href": "case-studies/stockouts/index.html#summary",
    "title": "A Hidden-Markov Model of Stockout Detection",
    "section": "",
    "text": "Bayesian Hidden Markov Model for detecting stockouts in sales data with no explicit inventory information. A synthetic dataset is generated with steady latent demand and known stockout periods to demonstrate the principle. Realized sales are treated as true demand realizations except during stockouts, which structurally force sales to zero and hide actual demand. The model treats sales as emissions from two hidden states - regular sales and stockout - with state transitions governed by a Markov process. Inference is performed in Stan using MAP estimation and the Viterbi algorithm to recover the most likely stockout periods. The result shows how probabilistic state-switching can uncover unobserved stockouts that distort sales data."
  },
  {
    "objectID": "case-studies/stockouts/index.html#simple-hmm",
    "href": "case-studies/stockouts/index.html#simple-hmm",
    "title": "A Hidden-Markov Model of Stockout Detection",
    "section": "1. Simple HMM",
    "text": "1. Simple HMM\nüîó [Notebook]"
  },
  {
    "objectID": "contact.html",
    "href": "contact.html",
    "title": "Get in touch",
    "section": "",
    "text": "If you‚Äôd like to discuss a potential project, collaboration, or have a question about statistical modeling, forecasting, or data science ‚Äî feel free to reach out.\nYou can also find me on:\n\n\n\n\n\n\n\n\nFull Name \nEmail Address \nMessage\n\n\n\nSend message"
  },
  {
    "objectID": "contact.html#drop-me-a-note",
    "href": "contact.html#drop-me-a-note",
    "title": "Get in touch",
    "section": "",
    "text": "If you‚Äôd like to discuss a potential project, collaboration, or have a question about statistical modeling, forecasting, or data science ‚Äî feel free to reach out.\nYou can also find me on:"
  },
  {
    "objectID": "projects/index.html",
    "href": "projects/index.html",
    "title": "Projects",
    "section": "",
    "text": "Selected examples of data science work. Details are anonymized to protect client confidentiality.\n\n\n\n\n\n\n\n\n\nNo matching items\n\n Back to top"
  },
  {
    "objectID": "case-studies/stockouts/stockouts.html",
    "href": "case-studies/stockouts/stockouts.html",
    "title": "Stockout Detection Based on Sales Data",
    "section": "",
    "text": "Code#library(dotenv)\n#library(digest)\nlibrary(tidyverse)\nlibrary(magrittr)\n#library(lubridate)\nlibrary(pscl)\nCode# simulates stockouts (we're assuming only full stockouts, no partial ones)\ngenerate_data &lt;- function(n_days, lambda_base, stockouts_idx, seed = 12)\n{\n    # use average lambda; no seasonality for now\n    lambda_day &lt;- rep(lambda_base, n_days) #+ (1:n_days)\n\n    # simulate latent sales, what would be realized if \n    set.seed(seed)\n    latent_demand_day &lt;- rpois(n_days, lambda_day)\n\n    # create stockout indicator \n    inventory_sufficient &lt;- rep(T, n_days)\n    valid_stockouts_idx &lt;- stockouts_idx[stockouts_idx &lt; n_days]\n    inventory_sufficient[valid_stockouts_idx] &lt;- F\n  \n    # we'll assume that stockouts don't increase sales afterwards \n    sales &lt;- latent_demand_day * inventory_sufficient\n\n    state &lt;- ifelse(inventory_sufficient, 'regular sales', 'stockout')\n    state %&lt;&gt;% factor(levels = c('stockout', 'regular sales')) \n    \n    data.frame( day = 1:n_days, sales = sales, state = state ) # stockout = !inventory_sufficient, \n}"
  },
  {
    "objectID": "case-studies/stockouts/stockouts.html#summary",
    "href": "case-studies/stockouts/stockouts.html#summary",
    "title": "Stockout Detection Based on Sales Data",
    "section": "Summary",
    "text": "Summary\nThis notebook demonstrates how to detect hidden stockouts in sales data when no explicit inventory records are available. Using a synthetic dataset with steady latent demand and known stockout periods, it shows how realized sales can drop to zero during stockouts, masking the true demand signal. A Bayesian Hidden Markov Model treats daily sales as emissions from two hidden states ‚Äî regular sales and stockout ‚Äî with transitions governed by a Markov process. Inference is performed in Stan using MAP estimation and the Viterbi algorithm to recover the most likely stockout periods. The approach illustrates how probabilistic state-switching can identify unobserved stockouts that distort sales-based analyses."
  },
  {
    "objectID": "case-studies/stockouts/stockouts.html#introduction",
    "href": "case-studies/stockouts/stockouts.html#introduction",
    "title": "Stockout Detection Based on Sales Data",
    "section": "Introduction",
    "text": "Introduction\nIn this notebook, we tackle a common data problem in analyzing retail data: Realized sales don‚Äôt always reflect the actual demand for a product. This is because when a product goes out of stock, the realized sales are artificially forced to zero, while the latent demand remains what it would have been.\nThis breaks the direct link between what is observed and what you‚Äôd want to use in an analysis or a forecast. If all realized sales are treated as if they came directly from the latent demand distribution, true demand will be underestimated.\nThis is a problem, because without explicit inventory data, the only clue about stockouts is that observed sales deviate from what the latent demand distribution could plausibly be."
  },
  {
    "objectID": "case-studies/stockouts/stockouts.html#simulating-sales-with-stockouts",
    "href": "case-studies/stockouts/stockouts.html#simulating-sales-with-stockouts",
    "title": "Stockout Detection Based on Sales Data",
    "section": "Simulating Sales With Stockouts",
    "text": "Simulating Sales With Stockouts\nWe generate synthetic sales data with steady demand and no seasonality, including a single stockout period to demonstrate the issue and its solution.\nThe function generate_data() below simulates:\n\nRegular sales: The realized sales depend entirely on the latent demand, and are simulated as draws from a Poisson distribution.\nStockout: Periods with no inventory, causing realized sales to drop to zero.\n\nThis gives us a clean example where we know which zeros are stockouts and which are a part of regular sales.\n\nCodedf &lt;- generate_data( n_days = 90, lambda_base = 3, \n                     stockouts_idx = c(30:36, 71:80), #, 100:120, 150:163, 200:218, 250:260)\n                   )\n\n\nThe plot shows flatlined realized during the stockout period, despite a constant latent demand distribution.\n\nCodedf %&gt;% ggplot(aes(day, sales, group=1, color = state)) + \n                    geom_line(color=\"darkgrey\") + geom_point() + \n                    theme_bw() + theme(legend.position = \"top\") + \n                    guides(color = guide_legend(title = NULL)) + \n                    ylab(\"sales quantity\") + \n                    ggtitle(\"Sales Over Time\") +\n                    theme(plot.title = element_text(hjust = 0.5))"
  },
  {
    "objectID": "case-studies/stockouts/stockouts.html#detecting-stockouts",
    "href": "case-studies/stockouts/stockouts.html#detecting-stockouts",
    "title": "Stockout Detection Based on Sales Data",
    "section": "Detecting Stockouts",
    "text": "Detecting Stockouts\nSo how do we detect stockouts? In order to do that, we first need to determine (i) the number of stockout days, and (ii) identify stretches of zeros the duration of which total approximately that length.\nDetermining the number of stockout days is relatively straightforward under the relatively plausible assumption that sales quantities are at least approximately follow the negaive binomial distribution: As is visible in the histogram below, the number of \\(0\\)s is unexpectedly large given the rest of the distribution. It should be smaller than the number of \\(1\\)s, but it is larger.\n\nCodedf %&gt;% ggplot(aes(sales)) + #, fill = state\n                    geom_histogram(bins = 30) + theme_bw() + theme(legend.position = \"top\") + \n                    guides(fill = guide_legend(title = NULL)) + \n                    xlab(\"sales quantity\") + \n                    scale_x_continuous(breaks=0:12) + \n                    ggtitle(\"Distribution of Sales Volume\") +\n                    theme(plot.title = element_text(hjust = 0.5))\n\n\nWe can use a zero-inflated negative binomial model to estimate a percentage of excess zeroes: One (simplified) way to think about it is that we fit a negative binomial distribution to all the non-zero quantites, and based on this estimated distribution determine the percentage of expected zeroes. The percentage above that are likely to be excess zeroes due to stockouts.\n\nCodemodel &lt;- pscl::zeroinfl(sales ~ 1 | 1, data = df, dist = \"negbin\")\nmu_hat &lt;- exp( coef(model)[\"count_(Intercept)\"] )\ntheta_hat &lt;- model$theta\nzi_coef &lt;- coef(model)[[\"zero_(Intercept)\"]]\nexcess_zero_prob &lt;- plogis(zi_coef)\n\ndf_zinb_mass &lt;- data.frame(\n  n = c(0,                0:9),\n  p = c(excess_zero_prob,  dnbinom(0:9, size = theta_hat, mu = mu_hat)*(1-excess_zero_prob)),\n  excess_zeros = ifelse(0:10==0, \"excess zeroes\", \"regular sales\")\n)\n\np1 &lt;- df_zinb_mass %&gt;% ggplot(aes(x = n, y = p, fill = excess_zeros)) +\n                    geom_bar(stat = \"identity\", width = 0.3) + theme_bw() + theme(legend.position = \"top\") + \n                    guides(fill = guide_legend(title = NULL)) + \n                    xlab(\"sales quantity\") + \n                    scale_x_continuous(breaks=0:12)\n\np2 &lt;- df %&gt;% group_by(sales, state) %&gt;% count() %&gt;% \n                    ggplot(aes(x = sales, y = n, fill = state)) + \n                    geom_bar(stat = \"identity\", width = 0.3) + theme_bw() + theme(legend.position = \"top\") + \n                    guides(fill = guide_legend(title = NULL)) + \n                    xlab(\"sales quantity\") + \n                    scale_x_continuous(breaks=0:12)\n\nggpubr::ggarrange(p1, p2 )\n\n\nHowever, while this method helps us understand approximately what proportion of days are affected by stockous, it doesn‚Äôt help us determine when exactly the stockouts occur. Some of the zeros are absolutely expected, even without a stockout."
  },
  {
    "objectID": "case-studies/stockouts/stockouts.html#a-probabilistic-model-of-stockouts",
    "href": "case-studies/stockouts/stockouts.html#a-probabilistic-model-of-stockouts",
    "title": "Stockout Detection Based on Sales Data",
    "section": "A Probabilistic Model of Stockouts",
    "text": "A Probabilistic Model of Stockouts\nThe key to a good stockout detection algorithm is that we need to posit stockouts where zeroes occur rather unexpectedly. This can happen when sales suddenly drop to zero\n\nduring a period with relatively high sales before and after the zero-sales stretch\nwhen sales have historically been rather high, or when\nwhen the number of successive zeroes is too long to be a coincidence during regular sales.\n\nTo detect such stockouts, we develop a probabilistic model based on the following principles:\n\nThe latent demand follows a specific distribution (e.g., Negative Binomial with \\(\\mu\\) and \\(\\theta\\)) that governs how many units would be sold if the stock were unlimited.\nThe parameter \\(\\mu\\) may depend on seasonal factors and may incorporate a trend.\nThe realized sales on any given day is either the latent demand, or \\(0\\), such as during a stockout.\nOn any given day, the system is in a particular state (stockout or regular sales) and transitions between states are each associated with specific probabilities.\n\nDetails\nWe model the phenomenon using a Hidden Markov Model with the structure in the diagram below: Each day is spent to be in one of two hidden states:\n\nRegular sales state (\\(R\\)): realized sales come from the latent demand distribution.\nStockout state (\\(S\\)): realized sales must be zero. The latent demand distribution is irrelevant because you can‚Äôt fulfill any of it.\n\n\n\nState Diagram of the Stockout HMM.\n\nTransitions between states are governed by a Markov process. The probability of staying in the regular sales state on day \\(t+1\\), given one was in that state on day \\(t\\) is \\(\\pi_{RR}\\).\nThe forward algorithm computes the likelihood of your entire observed series by marginalizing over all possible state sequences, given the emission distributions:\n\nIn the regular state, emissions come from the latent demand distribution, a negative binomial with parameters to be estimated.\nIn the stockout state, the emission is always zero.\n\nThe HMM is implemented as a Bayesian model in Stan, and we use MAP estimation to first obtain parameter estimates for the transition probabilites and the negative binomial parameters, and then use the Viterbi algorithm generating the most likely sequence of states generating the observed sequence of sales.\n\nCodelibrary(cmdstanr)\n\n# Compile the custom Stan model\nmodel &lt;- cmdstan_model(\"./stockouts_v1.stan\")\n\ndata_stan &lt;- list(n_days = nrow(df), sales_qty = df$sales )\n\nopt &lt;- model$optimize(\n  data = data_stan,\n  seed = 123,\n  iter = 5000\n)\n\n#opt$summary()\n\ngq &lt;- model$generate_quantities(\n  fitted_params = opt$draws(),\n  data = data_stan\n)\n\nlikely_state &lt;- gq$summary()$mean\ndf$likely_state &lt;- ifelse(likely_state == 2, \"regular sales\", \"stockout\")\n\n\nThe plot below shows sales sequence generated above, with the red bands indicating the periods that have been identified as stockouts.\n\nCodedatected_stockouts &lt;- df %&gt;%\n          mutate(prev_likely_state = lag(likely_state, default = \"0\"),\n                 change = (likely_state != prev_likely_state),\n                 segment_id = cumsum(change)) %&gt;% \n          filter( likely_state == \"stockout\" ) %&gt;% \n          group_by(segment_id) %&gt;% \n          summarize( day_start = min(day), day_end = max(day) )\n\ndf %&gt;% ggplot(aes(day, sales, group=1, color = state)) + \n                    geom_line(color=\"darkgrey\") + geom_point() + \n                    theme_bw() + theme(legend.position = \"top\") + \n                    guides(color = guide_legend(title = NULL)) + \n                    ylab(\"sales quantity\") + \n                    ggtitle(\"Sales Over Time\") +\n                    theme(plot.title = element_text(hjust = 0.5)) +\n                    geom_rect(\n                      data = datected_stockouts,\n                      inherit.aes = FALSE,\n                      aes( xmin = day_start, xmax = day_end,\n                           ymin = -Inf, ymax = Inf\n                      ),\n                      fill = \"red\",\n                      alpha = 0.2\n                    )"
  },
  {
    "objectID": "case-studies/time_series_analysis_1/ts_2_analysis.html",
    "href": "case-studies/time_series_analysis_1/ts_2_analysis.html",
    "title": "Bayesian Analysis of Sales",
    "section": "",
    "text": "This notebook demonstrates the use of Bayesian inference for sales forecasting using various probabilistic programming techniques. We will use the numpyro library to define and fit our models, and plotnine for visualization.\n\nBayesian inference allows us to incorporate prior knowledge and quantify uncertainty in our predictions. This notebook will guide you through the process of building a Bayesian model for sales forecasting, fitting the model using Markov Chain Monte Carlo (MCMC) and Stochastic Variational Inference (SVI), and visualizing the results.\n\n\nCodelibrary(reticulate)\n\n\n\nCodeimport os\nimport sys\n\n# Set XLA_FLAGS before JAX is imported\nos.environ[\"XLA_FLAGS\"] = \"--xla_force_host_platform_device_count=8\"\n\n\n\nCodeimport polars as pl\nimport pandas as pd\nimport numpy as np\nfrom plotnine import ggplot, aes, geom_point, geom_line, labs, theme_minimal, theme_bw, scale_x_continuous, scale_x_discrete, scale_x_datetime\nimport patsy\n\nimport jax\nimport jax.numpy as jnp\nimport jax.random as random\nfrom jax.scipy.special import expit, logit\n\nimport numpyro\nimport numpyro.distributions as dist\n#from numpyro.infer import SVI, Trace_ELBO, Predictive\nfrom numpyro.infer import MCMC, NUTS, MCMC, NUTS #, SVI, Trace_ELBO\nimport arviz as az\n\nimport polars as pl\nfrom plotnine import theme, guides, guide_legend\nimport pickle\n#jax.config.update(\"jax_enable_x64\", True)  # Enable float64 by default\n\n\n\nIn this section, we define the Bayesian model used for sales forecasting. The model incorporates various components such as random walk for the latent state, day-of-the-week effects, day-of-the-year effects, and price elasticity. The model is implemented using the numpyro library, which allows for efficient and scalable Bayesian inference.\n\nThese auxiliary functions are essential for data preprocessing and transformation:\n\n\nperiodic_rbf: Computes a periodic Gaussian radial basis function (RBF).\n\ncompute_doy_basis: Computes 12 periodic Gaussian basis functions for seasonal effects.\n\nread_data: Reads and preprocesses the sales data from a CSV file.\n\ninit_values: Initializes values for the model parameters.\n\n\nCode# Define a periodic Gaussian radial basis function (RBF)\ndef periodic_rbf(x, mu, sigma):\n    \"\"\"\n    Computes a periodic Gaussian radial basis function (RBF).\n    \n    Args:\n        x: Scaled day-of-year values (range [0,1]).\n        mu: Center of the Gaussian basis function.\n        sigma: Controls the spread of the Gaussian.\n    \n    Returns:\n        RBF values preserving periodicity.\n    \"\"\"\n    # compute cyclic distance to mu\n    periodic_distance = jnp.minimum(jnp.abs(x - mu), 1 - jnp.abs(x - mu))\n    # compute RBF value\n    return jnp.exp(- (periodic_distance ** 2) / (2 * sigma ** 2))\n\ndef compute_doy_basis(yday_fraction, sigma = 30/365.25, n_centers = 12):\n    \"\"\"\n    Computes 12 periodic Gaussian basis functions for seasonal effects.\n    \n    Args:\n        yday_fraction: Normalized day of the year (range [0,1]).\n        yday_factor: Scaling factor for basis function width.\n    \n    Returns:\n        A JAX array with 12 columns representing the 12 monthly basis functions.\n    \"\"\"\n    # Define centers of Gaussian basis functions\n    month_centers = jnp.linspace( 1/(2*n_centers), 1-1/(2*n_centers), n_centers)\n    \n    # Generate an array of shape (length of input, 12) with the RBF values\n    doy_basis = jnp.stack([periodic_rbf(yday_fraction, mu, sigma) for mu in month_centers], axis=-1)\n\n    # Subtract each row's mean to enforce sum-to-zero constraint\n    doy_basis_centered = doy_basis - jnp.mean(doy_basis, axis=-1, keepdims=True)\n    \n    return doy_basis_centered\n\ndef read_data(fname, n_rows=None):\n    \"\"\"\n    Reads and preprocesses the sales data from a CSV file.\n    \n    Args:\n        fname: The filename of the CSV file containing the sales data.\n    \n    Returns:\n        A dictionary with the following keys:\n            - sales: An array of sales data.\n            - log_price: An array of log-transformed prices.\n            - wday: An array of day-of-the-week values.\n            - yday_fraction: An array of normalized day-of-the-year values.\n    \"\"\"\n    # Read the CSV file using polars\n    df = pl.read_csv(fname)\n    \n    # Keep only the first n_rows if specified\n    if n_rows is not None:\n        df = df.head(n_rows)\n    \n    # Convert the 'date' column to date type\n    df = df.with_columns(pl.col(\"date\").str.to_date())\n\n    # Extract sales, and log price data as a numpy arrays\n    sales = df[\"sales\"].to_numpy()\n    log_price = df[\"log_price\"].to_numpy()\n    \n    # Extract day-of-the-week values\n    wday = df[\"date\"].dt.weekday().to_numpy() # set offset to 0\n            \n    # Extract day-of-the-year values\n    yday = df[\"date\"].dt.ordinal_day().to_numpy()\n    \n    # Determine if the year is a leap year\n    is_leap_year = df[\"date\"].dt.is_leap_year().to_numpy()\n    \n    # Normalize day-of-the-year values\n    yday_fraction = yday / (365 + is_leap_year)\n    \n    # Return the preprocessed data as a dictionary\n    return {\n        \"date\": df[\"date\"].to_numpy(),\n        \"sales\": sales,\n        \"log_price\": log_price,\n        \"wday\": wday,\n        \"yday_fraction\": yday_fraction\n    }\n\ndef interpolate(x, downsampling_factor, n_out):\n    \"\"\"\n    \"\"\"\n    idx_n_weight = jnp.array(range(0, n_out))/jnp.float64(downsampling_factor)\n    idx_1 = jnp.array( jnp.floor(idx_n_weight), dtype=int)\n    idx_2 = jnp.array( jnp.ceil(idx_n_weight), dtype=int)\n    weight_2 = idx_n_weight - idx_1\n\n    state_before = x[idx_1]\n    state_after  = x[idx_2]\n\n    return (1-weight_2)*state_before + weight_2*state_after\n\ndef chunked_mean(x, n_chunk):\n    n = x.shape[0]\n    pad_size = -n % n_chunk # compute padding needed to make k a multiple of n\n    x_padded = jnp.pad(array = x, pad_width = (0, pad_size), mode = 'edge') # pad at the end\n    return x_padded.reshape(-1, n_chunk).mean(axis=1)\n\ndef chunked_sum(x, n_chunk):\n    n = x.shape[0]\n    pad_size = -n % n_chunk # compute padding needed to make k a multiple of n\n    x_padded = jnp.pad(array = x, pad_width = (0, pad_size)) # pad at the end\n    return x_padded.reshape(-1, n_chunk).sum(axis=1)\n\n\n\nCode# Creates a simple plot using plotnine\ndef plot_function(x, y, title, xlab, ylab):\n    # Convert x to numpy array\n    x = np.array(x)\n    \n    # Check if y is a callable function\n    if callable(y):\n        # If y is a function, apply it to x and create a DataFrame\n        df = pd.DataFrame({\"x\": x, \"y\": y(x)})\n    else:\n        # If y is not a function, create a DataFrame directly\n        df = pd.DataFrame({\"x\": x, \"y\": y})        \n\n    # Create the plot using ggplot\n    plot = (ggplot(df, aes(x=\"x\", y=\"y\")) + geom_line() + labs(title=title, x=xlab, y=ylab) + theme_bw())\n    \n    # Return the plot\n    return plot\n\n\n\n\n\nSales are modeled using a stochastic Poisson process, where the expected rate \\(\\lambda_t\\) evolves over time.\nThe latent sales rate follows a random walk, allowing it to drift nonstationarily.\n\nSeasonal components (day-of-the-week and annual patterns) adjust for structured demand variations.\n\nPrice elasticity is explicitly modeled, ensuring sensitivity to pricing dynamics.\nThe model is implemented in numpyro, enabling scalable Bayesian inference.\n\nWe model the sales time series as a stochastic process where the underlying rate of sales evolves over time. This evolution follows a random walk structure, but with systematic adjustments for covariates such as price, day-of-the-week effects, and day-of-the-year effects. The rate of sales \\(\\lambda_t\\) on day \\(t\\) is a function of captures (i) systematic covariate effects (\\(z_t\\)), (ii) a global baseline (\\(\\mu_\\tau\\)), and (iii) the latent dynamic component (\\(\\tau_t\\)).\n\\[\nlog~\\lambda_t = z_t + \\mu_\\tau + \\tau_t\n\\]\n\nThe baseline sales level \\(\\tau_t\\) follows a random walk. Because all contrast matrices for structured effects are centered, \\(\\mu_\\tau + \\tau_t\\) can be interpreted as the average latent sales rate on \\(\\tau_t\\).\n\\[\n\\tau_t = \\tau_{t-1} + \\delta_t, \\quad \\delta_t \\sim \\mathcal{N}(0, \\sigma_\\tau)\n\\]\nwith:\n\\[\n\\mu_\\tau \\sim \\text{Exponential}(1), \\quad \\sigma_\\tau \\sim \\mathcal{N}(1)\n\\]\n\nWe further accounted for systematic effects of (i) day of the week, (ii) day of the year, and (iii) price.\n\nFor day of the week effects, we used a contrast matrix \\(\\mathbf{C}_{\\text{wday}}\\) with sliding differences.\nFor day of the year effects, we used a matrix of Gaussian radial basis functions \\(\\mathbf{B}_{\\text{yday}}\\).\nPrice elasticity is modelled using a centered log price\n\nSimilarly, the day-of-the-year effects are modeled using a seasonality basis matrix \\(\\mathbf{B}_{\\text{yday}}\\), which represents periodic seasonal patterns using Gaussian radial basis functions (RBFs).\n\n\nDay-of-the-week effects:\n\n\\[\n  zw_t = \\mathbf{C}_{\\text{wday}} \\cdot \\beta_{\\text{wday}}, \\quad \\beta_{\\text{wday}} \\sim \\mathcal{N}(0, 1)\n\\]\n\n\nDay-of-the-year effects:\n\n\\[\n  zy_t = \\mathbf{B}_{\\text{yday}} \\cdot \\beta_{\\text{yday}}, \\quad \\beta_{\\text{yday}} \\sim \\mathcal{N}(0, 1)\n\\]\n\n\nPrice elasticity:\n\n\\[\n  ze_t = \\text{log\\_price\\_centered} \\cdot e, \\quad \\log(-e) \\sim \\mathcal{N^{+}}(0, 1)\n\\]\n\n\nSum of structural effects:\n\n\\[\n  z_t = zw_t + zy_t + ze_t\n\\]\n\nObserved sales are assumed to follow a Poisson distribution, ensuring discrete, non-negative observations:\n\\[\nS_t \\sim \\text{Poisson}(\\lambda_t)\n\\]\n\nCodedef model_local_level_poisson(sales: jnp.array, log_price_centered: jnp.array, wday: jnp.array, yday_fraction: jnp.array, \n                              contrasts_sdif_t: jnp.array, contrasts_wday: jnp.array, contrasts_yday: jnp.array, \n                              downsampling_factor = 1):\n    \"\"\"\n    \"\"\"\n\n    n_obs = len(sales)\n    n_states = contrasts_sdif_t.shape[0]\n \n    def sample_random_walk(contrasts_sdif_t, n_states):\n        log_sigma = numpyro.sample(\"log_sigma\", dist.Gumbel(0, 5))\n        sigma = numpyro.deterministic(\"sigma\", jnp.exp(log_sigma))\n        log_state_mean = numpyro.sample(\"log_state_mean\", dist.Normal(0, 5)) # to-do: add an average drift term, as well as potentially an additional parameter governing drift dynamics \n        log_state_delta = numpyro.sample( \"log_state_delta\", dist.Normal(0, 1), sample_shape=(n_states-1,))\n        log_state_base = numpyro.deterministic(\"log_state_base\", jnp.dot(contrasts_sdif_t, log_state_delta) * sigma + log_state_mean )\n        return log_state_base\n\n    def sample_downsampled_random_walk(contrasts_sdif_t, n_obs, n_states):\n        log_state_base_downsampled = sample_random_walk(contrasts_sdif_t, n_states)\n        log_state_base = interpolate(log_state_base_downsampled, downsampling_factor, n_obs)\n        return log_state_base\n        \n    def sample_wday_effect(contrasts_wday, wday):\n        # Prior for day-of-the-week effects (6 coefficients)\n        wday_coefficients = numpyro.sample(\"wday_coefficients\", dist.Normal(0, 1), sample_shape=(6,))\n\n        # Compute wday effect per observation (sum-to-zero constraint applied via contrasts)\n        wday_effects = jnp.dot(contrasts_wday, wday_coefficients)\n        return jnp.array(wday_effects[wday-1])\n\n    def sample_yday_effect(contrasts_yday, yday_fraction):\n        # Prior for yearly seasonality effects (12 coefficients)\n        yday_coefficients = numpyro.sample(\"yday_coefficients\", dist.Normal(0, 1), sample_shape=(12,))\n        return jnp.dot(contrasts_yday, yday_coefficients)\n\n    def sample_price_effect(log_price_centered):\n        # Prior for price elasticity\n        elasticity_pos = numpyro.sample( \"elasticity_pos\", dist.HalfNormal(10) )\n        elasticity = numpyro.deterministic(\"elasticity\", -1 * elasticity_pos)\n        return log_price_centered * elasticity\n\n    # Sample random walk    \n    if n_obs == n_states:\n        log_state_base = sample_random_walk(contrasts_sdif_t, n_states)\n    else:\n        log_state_base = sample_downsampled_random_walk(contrasts_sdif_t, n_obs, n_states)\n\n    # Sample day-of-the-week effects\n    wday_effect = sample_wday_effect(contrasts_wday, wday)\n\n    # Sample day-of-the-year effects\n    yday_effect = sample_yday_effect(contrasts_yday, yday_fraction)\n\n    # Sample elasticity effect\n    price_effect = sample_price_effect(log_price_centered)\n\n    # Compute state\n    state = numpyro.deterministic(\"state\", jnp.exp( log_state_base  + yday_effect + wday_effect + price_effect )) #   # \n\n    # Compute log-likelihood for poisson emissions\n    numpyro.sample(\"sales\", dist.Poisson(rate=state), obs=sales) # to-do: create a Poisson distribution paramaterized by log-rate, as in the Stan manual \n\n\n\nWe use the run_nuts function to fit the model to our sales data. The function leverages the No-U-Turn Sampler (NUTS) from the numpyro library to perform MCMC sampling. Because the model has a large number of latent parameters, initialization to sensible start values is key.\n\nIn order to fit the model, the functions below are used:\n\nprepare_model_arguments: Transforms the data into a format required by the model, including sales data, log-transformed prices, day-of-the-week values, and normalized day-of-the-year values.\ninit_values: Finds sensible start values for the model parameters which, in this case is crucial for the convergence of the MCMC algorithm.\nrun_nuts: Given a dataset, it calls the NUTS sampler to perform MCMC sampling.\n\n\nCodedef init_values(sales: jnp.array, log_price_centered: jnp.array, wday, yday_fraction: jnp.array, downsampling_factor = 1):\n    \"\"\"\n    \"\"\"\n    epsilon = 0.001\n    log_state_est = jnp.log(sales + epsilon)\n    log_state_mean_est = jnp.mean(log_state_est)\n    log_state_delta_est = jnp.diff(log_state_est)\n    if downsampling_factor &gt; 1:\n        log_state_delta_est = chunked_sum(log_state_delta_est, downsampling_factor)\n        \n    log_state_delta_sd_est = jnp.std(log_state_delta_est)\n\n    return {\n        \"log_sigma\": jnp.log( log_state_delta_sd_est ),\n        \"log_state_mean\": log_state_mean_est,\n        \"log_state_delta\": log_state_delta_est,\n        \"wday_coefficients\": jnp.array([0.0]*6),\n        \"yday_coefficients\": jnp.array([0.0]*12),\n        \"log_elasticity\": jnp.array([0.0])\n    }\n\n\n\nCodedef prepare_model_arguments(sales: jnp.array, log_price: jnp.array, wday: jnp.array, yday_fraction: jnp.array, downsampling_factor = 1):\n    \"\"\" \n    Prepares the arguments required for the model.\n    \n    Args:\n        sales: Array of sales data.\n        log_price: Array of log-transformed prices.\n        wday: Array of day-of-the-week values.\n        yday_fraction: Array of normalized day-of-the-year values.\n        downsampling_factor: Factor by which to downsample the data.\n    \n    Returns:\n        A tuple containing initialized values for the model parameters and the model arguments.\n    \"\"\"    \n    n_obs = len(sales)\n    \n    # Determine the number of states based on the downsampling factor\n    if downsampling_factor == 1:\n        n_states = n_obs\n    else:\n        n_states = int(np.ceil(n_obs / downsampling_factor) + 1)\n    \n    # Define contrast matrix for random walk (T coefficients, sum-to-zero constraint)\n    contrasts_sdif_t = patsy.contrasts.Diff().code_without_intercept(range(0, n_states)).matrix\n\n    # Define contrast matrix for day-of-the-week effects (6 coefficients, sum-to-zero constraint)\n    contrasts_wday = patsy.contrasts.Diff().code_without_intercept(range(0, 7)).matrix  # 7 days ‚Üí 6 contrasts\n\n    # Compute yday effect per observation (sum-to-zero constraint applied via contrasts)\n    contrasts_yday = compute_doy_basis(yday_fraction, sigma=30/365.25, n_centers=12) # to-do: do a very approximate calibration of the RBF width parameter sigma, using something like a spline for the long term trend + RBF seasonality\n\n    # Compute centered log price differences\n    log_price_centered = log_price - jnp.mean(log_price)\n\n    # Set up the model parameters\n    model_arguments = {\n        'sales': sales,\n        'log_price_centered': log_price_centered,\n        'wday': jnp.array(wday, dtype=int),\n        'yday_fraction': yday_fraction,\n        'downsampling_factor': downsampling_factor,\n        'contrasts_sdif_t': contrasts_sdif_t,\n        'contrasts_wday': contrasts_wday,\n        'contrasts_yday': contrasts_yday\n    }\n    \n    # Prepare initial values for parameters \n    init_params = init_values(sales, log_price_centered, wday, yday_fraction, downsampling_factor)\n\n    return init_params, model_arguments\n\n\n\nCodedef run_nuts(sales: jnp.array, log_price: jnp.array, wday, yday_fraction: jnp.array, downsampling_factor = 1, n_chains = 1, num_warmup=1_000, num_samples=1_000, step_size=0.01, max_tree_depth=8):\n    \"\"\" Runs NUTS MCMC inference on the model \n    \"\"\"\n    # Initialize random number generator key\n    rng_key = random.PRNGKey(0)\n    \n    # Get the number of observations\n    n_obs = len(sales)\n    \n    # Prepare model arguments and initial parameter values\n    init_params, model_arguments = prepare_model_arguments(sales = sales, log_price = log_price, wday = wday, yday_fraction = yday_fraction, downsampling_factor = downsampling_factor)\n\n    # Split the random number generator key\n    rng_key, rng_key_ = random.split(rng_key)\n\n    # Set the number of chains for parallel sampling\n    numpyro.set_host_device_count(n_chains)\n\n    # Define the model to be used\n    reparam_model = model_local_level_poisson\n    \n    # Initialize the NUTS kernel with the specified step size and tree depth\n    kernel = NUTS(reparam_model, step_size=step_size, max_tree_depth=max_tree_depth)\n    \n    # Initialize the MCMC sampler with the NUTS kernel\n    mcmc = MCMC(kernel, num_warmup=num_warmup, num_samples=num_samples, num_chains=n_chains)\n    \n    # Run the MCMC sampler\n    mcmc.run(rng_key_, **model_arguments) # disable init values: init_params=init_params\n\n    # Return the fitted MCMC object\n    return mcmc, model_arguments\n\n\n\n\nWe fit the model to the synthetic dataset using the run_nuts function. The model is fitted using the No-U-Turn Sampler (NUTS) from the numpyro library, with 4 chains, 1,000 warmup iterations, and 1,000 sampling iterations. The step size is set to 0.01, and the maximum tree depth is 8. The fitted model is stored in the m_fit variable.\nOn CPU, the process takes about 2 minutes.\n\n\nCode# read in the synthetic sales data\ndata = read_data(\"sales_synthetic.csv\")\n\n\n\nCodewith open(\"sim_parameters.pkl\", \"rb\") as f:\n    sim_parameters = date_range, growth, growth_plus_rw, scale_factor, wday, weekly_seasonality, yearly_seasonality = pickle.load(f)\n\n\n\nCode# Fit the model using NumPyro NUTS MCMC\nm_fit, model_arguments = run_nuts(data['sales'], data['log_price'], data['wday'], data['yday_fraction'], \n                                    downsampling_factor=7, n_chains=4, num_warmup=1_000, num_samples=1_000,\n                                    step_size=0.01, max_tree_depth=8)\n\n\n\n\nAll effective sample sizes are decent, which is a good sign. The Gelman-Rubin statistics are close to 1, indicating convergence. Inspection of trace plots are beyond the scope of this notebook.\nThe model successfully reconstructs all key features of the synthetic dataset.\n\n\n\nThe estimated random walk component closely follows the true trajectory of the combination long-term trend + random fluctuations in the synthetic data.\n\n\nCode# Let's look at the estimated random walk component of the model.\nsummary = az.summary(m_fit, var_names=[\"sigma\", \"log_state_delta\"], filter_vars=\"like\")\n\n\n\nCodepy$summary\n\n\n\nHere, we‚Äôll plot the estimated the random walk component, which also incorporates long term trends and growth or decline of sales. For present purposes, this is what amounts to irrelevant noise in the model.\n\n\nCode# Create a sequence of dates starting at data[\"date\"].min() the length of x['mean'], in steps of 7 days\nrw_states = az.summary(m_fit, var_names=[\"log_state_base\"], filter_vars=\"like\")[\"mean\"].to_numpy()\ndates = pd.date_range(start = data[\"date\"].min(), periods = len(rw_states), freq='7D')\n#p = plot_function(dates, np.exp(rw_states), \"Estimated Random Walk Component\", \"Date\", \"Sales\") # to-do: add uncertainty bands\n\ndf_1 = pl.DataFrame({'x': date_range, 'y': growth_plus_rw*scale_factor, 'var': 'Simulated Trend + Random Walk' })\ndf_2 = pl.DataFrame({'x': dates, 'y': np.exp(rw_states), 'var': 'Estimated Trend + Random Walk' })\ndf = pl.concat([df_1, df_2])\n\np = ggplot(df, aes(\"x\", \"y\", color = \"var\")) + geom_line() + theme_bw() + theme(legend_position='top') + guides(color=guide_legend(title=\"\"))\n_ = p.draw(show=True)\n\n\n\n\nThe model appears to correctly identifies weekly sales fluctuations.\n\n\nCodecoefs_wday = az.summary(m_fit, var_names=[\"wday_coefficients\"], filter_vars=\"like\")\n\n\n\nCodepy$coefs_wday\n\n\n\nCodewday_effect = jnp.dot(model_arguments[\"contrasts_wday\"], jnp.array(coefs_wday[\"mean\"]))\np = plot_function(range(0,7), wday_effect, \"Effect of Day of the Week\", \"Date\", \"Sales\") # to-do: add uncertainty bands\n\ndf_1 = pl.DataFrame({'x': range(0,7), 'y': weekly_seasonality - np.mean(weekly_seasonality), 'var': 'Simulated Weekly Seasonality' })\ndf_2 = pl.DataFrame({'x': range(0,7), 'y': wday_effect.tolist(), 'var': 'Estimated Weekly Seasonality' })\ndf = pl.concat([df_1, df_2])\n\np = ggplot(df, aes(\"x\", \"y\", color = \"var\")) + geom_line() + theme_bw() + theme(legend_position='top') + guides(color=guide_legend(title=\"\"))\n_ = p.draw(show=True)\n\n\n\n\nSeasonal peaks and troughs are largely accurately captured, with minor deviations.\n\n\nCodecoefs_yday = az.summary(m_fit, var_names=[\"yday_coefficients\"], filter_vars=\"like\")\n\n\n\nCodepy$coefs_yday\n\n\n\nCodeyday_effect = jnp.dot(model_arguments[\"contrasts_yday\"], jnp.array(coefs_yday[\"mean\"]))\n#p = plot_function(data[\"date\"], yday_effect, \"Yearly Seasonality\", \"Date\", \"Sales\") # to-do: add uncertainty bands\n\ndf_1 = pl.DataFrame({'x': date_range, 'y': yearly_seasonality - np.mean(yearly_seasonality), 'var': 'Simulated Yearly Seasonality' }).with_columns(\n    pl.col(\"x\").cast(pl.Date)\n)\ndf_2 = pl.DataFrame({'x': data[\"date\"].tolist(), 'y': yday_effect.tolist(), 'var': 'Estimated Yearly Seasonality' })\ndf = pl.concat([df_1, df_2])\n\np = ggplot(df, aes(\"x\", \"y\", color = \"var\")) + geom_line() + theme_bw() + theme(legend_position='top') + guides(color=guide_legend(title=\"\"))\n\n_ = p.draw(show=True)\n\n\n\n\nThe estimated elasticity coefficient is close enough to the true value (\\(-1.4\\)), though the \\(94\\%\\) credible interval is quite wide.\n\n\nCodesummary = az.summary(m_fit, var_names=[\"elasticity\"])\n\n\n\nCodepy$summary\n\n\n\nThis case study demonstrates how Bayesian modeling can effectively decompose sales variance into meaningful components, providing a structured way to analyze the underlying factors driving sales fluctuations. By applying this approach to a synthetic dataset, we validated the model‚Äôs ability to separate out long-term growth, seasonal effects, and price sensitivity while simultaneously quantifying uncertainty.\nThe key takeaways include:\n\nDecomposing complexity: The model successfully isolates different components influencing sales, making it easier to interpret real-world dynamics.\nQuantifying uncertainty: In addition to point estimates, Bayesian inference provides full posterior distributions, enabling better risk assessment.\nInformed decision-making: By accounting for all sources of variance, businesses can make more confident strategic decisions that explicitly consider uncertainty. For instance, price optimization can be performed on the entire posterior distribution of the estimate of the price elasticity of demand.\n\nThese findings highlight the advantages of probabilistic modeling in sales analysis, offering a flexible and interpretable method."
  },
  {
    "objectID": "case-studies/time_series_analysis_1/ts_2_analysis.html#introduction",
    "href": "case-studies/time_series_analysis_1/ts_2_analysis.html#introduction",
    "title": "Bayesian Analysis of Sales",
    "section": "",
    "text": "Bayesian inference allows us to incorporate prior knowledge and quantify uncertainty in our predictions. This notebook will guide you through the process of building a Bayesian model for sales forecasting, fitting the model using Markov Chain Monte Carlo (MCMC) and Stochastic Variational Inference (SVI), and visualizing the results."
  },
  {
    "objectID": "case-studies/time_series_analysis_1/ts_2_analysis.html#import-required-libraries-and-define-functions",
    "href": "case-studies/time_series_analysis_1/ts_2_analysis.html#import-required-libraries-and-define-functions",
    "title": "Bayesian Analysis of Sales",
    "section": "",
    "text": "Codelibrary(reticulate)\n\n\n\nCodeimport os\nimport sys\n\n# Set XLA_FLAGS before JAX is imported\nos.environ[\"XLA_FLAGS\"] = \"--xla_force_host_platform_device_count=8\"\n\n\n\nCodeimport polars as pl\nimport pandas as pd\nimport numpy as np\nfrom plotnine import ggplot, aes, geom_point, geom_line, labs, theme_minimal, theme_bw, scale_x_continuous, scale_x_discrete, scale_x_datetime\nimport patsy\n\nimport jax\nimport jax.numpy as jnp\nimport jax.random as random\nfrom jax.scipy.special import expit, logit\n\nimport numpyro\nimport numpyro.distributions as dist\n#from numpyro.infer import SVI, Trace_ELBO, Predictive\nfrom numpyro.infer import MCMC, NUTS, MCMC, NUTS #, SVI, Trace_ELBO\nimport arviz as az\n\nimport polars as pl\nfrom plotnine import theme, guides, guide_legend\nimport pickle\n#jax.config.update(\"jax_enable_x64\", True)  # Enable float64 by default"
  },
  {
    "objectID": "case-studies/time_series_analysis_1/ts_2_analysis.html#model",
    "href": "case-studies/time_series_analysis_1/ts_2_analysis.html#model",
    "title": "Bayesian Analysis of Sales",
    "section": "",
    "text": "In this section, we define the Bayesian model used for sales forecasting. The model incorporates various components such as random walk for the latent state, day-of-the-week effects, day-of-the-year effects, and price elasticity. The model is implemented using the numpyro library, which allows for efficient and scalable Bayesian inference.\n\nThese auxiliary functions are essential for data preprocessing and transformation:\n\n\nperiodic_rbf: Computes a periodic Gaussian radial basis function (RBF).\n\ncompute_doy_basis: Computes 12 periodic Gaussian basis functions for seasonal effects.\n\nread_data: Reads and preprocesses the sales data from a CSV file.\n\ninit_values: Initializes values for the model parameters.\n\n\nCode# Define a periodic Gaussian radial basis function (RBF)\ndef periodic_rbf(x, mu, sigma):\n    \"\"\"\n    Computes a periodic Gaussian radial basis function (RBF).\n    \n    Args:\n        x: Scaled day-of-year values (range [0,1]).\n        mu: Center of the Gaussian basis function.\n        sigma: Controls the spread of the Gaussian.\n    \n    Returns:\n        RBF values preserving periodicity.\n    \"\"\"\n    # compute cyclic distance to mu\n    periodic_distance = jnp.minimum(jnp.abs(x - mu), 1 - jnp.abs(x - mu))\n    # compute RBF value\n    return jnp.exp(- (periodic_distance ** 2) / (2 * sigma ** 2))\n\ndef compute_doy_basis(yday_fraction, sigma = 30/365.25, n_centers = 12):\n    \"\"\"\n    Computes 12 periodic Gaussian basis functions for seasonal effects.\n    \n    Args:\n        yday_fraction: Normalized day of the year (range [0,1]).\n        yday_factor: Scaling factor for basis function width.\n    \n    Returns:\n        A JAX array with 12 columns representing the 12 monthly basis functions.\n    \"\"\"\n    # Define centers of Gaussian basis functions\n    month_centers = jnp.linspace( 1/(2*n_centers), 1-1/(2*n_centers), n_centers)\n    \n    # Generate an array of shape (length of input, 12) with the RBF values\n    doy_basis = jnp.stack([periodic_rbf(yday_fraction, mu, sigma) for mu in month_centers], axis=-1)\n\n    # Subtract each row's mean to enforce sum-to-zero constraint\n    doy_basis_centered = doy_basis - jnp.mean(doy_basis, axis=-1, keepdims=True)\n    \n    return doy_basis_centered\n\ndef read_data(fname, n_rows=None):\n    \"\"\"\n    Reads and preprocesses the sales data from a CSV file.\n    \n    Args:\n        fname: The filename of the CSV file containing the sales data.\n    \n    Returns:\n        A dictionary with the following keys:\n            - sales: An array of sales data.\n            - log_price: An array of log-transformed prices.\n            - wday: An array of day-of-the-week values.\n            - yday_fraction: An array of normalized day-of-the-year values.\n    \"\"\"\n    # Read the CSV file using polars\n    df = pl.read_csv(fname)\n    \n    # Keep only the first n_rows if specified\n    if n_rows is not None:\n        df = df.head(n_rows)\n    \n    # Convert the 'date' column to date type\n    df = df.with_columns(pl.col(\"date\").str.to_date())\n\n    # Extract sales, and log price data as a numpy arrays\n    sales = df[\"sales\"].to_numpy()\n    log_price = df[\"log_price\"].to_numpy()\n    \n    # Extract day-of-the-week values\n    wday = df[\"date\"].dt.weekday().to_numpy() # set offset to 0\n            \n    # Extract day-of-the-year values\n    yday = df[\"date\"].dt.ordinal_day().to_numpy()\n    \n    # Determine if the year is a leap year\n    is_leap_year = df[\"date\"].dt.is_leap_year().to_numpy()\n    \n    # Normalize day-of-the-year values\n    yday_fraction = yday / (365 + is_leap_year)\n    \n    # Return the preprocessed data as a dictionary\n    return {\n        \"date\": df[\"date\"].to_numpy(),\n        \"sales\": sales,\n        \"log_price\": log_price,\n        \"wday\": wday,\n        \"yday_fraction\": yday_fraction\n    }\n\ndef interpolate(x, downsampling_factor, n_out):\n    \"\"\"\n    \"\"\"\n    idx_n_weight = jnp.array(range(0, n_out))/jnp.float64(downsampling_factor)\n    idx_1 = jnp.array( jnp.floor(idx_n_weight), dtype=int)\n    idx_2 = jnp.array( jnp.ceil(idx_n_weight), dtype=int)\n    weight_2 = idx_n_weight - idx_1\n\n    state_before = x[idx_1]\n    state_after  = x[idx_2]\n\n    return (1-weight_2)*state_before + weight_2*state_after\n\ndef chunked_mean(x, n_chunk):\n    n = x.shape[0]\n    pad_size = -n % n_chunk # compute padding needed to make k a multiple of n\n    x_padded = jnp.pad(array = x, pad_width = (0, pad_size), mode = 'edge') # pad at the end\n    return x_padded.reshape(-1, n_chunk).mean(axis=1)\n\ndef chunked_sum(x, n_chunk):\n    n = x.shape[0]\n    pad_size = -n % n_chunk # compute padding needed to make k a multiple of n\n    x_padded = jnp.pad(array = x, pad_width = (0, pad_size)) # pad at the end\n    return x_padded.reshape(-1, n_chunk).sum(axis=1)\n\n\n\nCode# Creates a simple plot using plotnine\ndef plot_function(x, y, title, xlab, ylab):\n    # Convert x to numpy array\n    x = np.array(x)\n    \n    # Check if y is a callable function\n    if callable(y):\n        # If y is a function, apply it to x and create a DataFrame\n        df = pd.DataFrame({\"x\": x, \"y\": y(x)})\n    else:\n        # If y is not a function, create a DataFrame directly\n        df = pd.DataFrame({\"x\": x, \"y\": y})        \n\n    # Create the plot using ggplot\n    plot = (ggplot(df, aes(x=\"x\", y=\"y\")) + geom_line() + labs(title=title, x=xlab, y=ylab) + theme_bw())\n    \n    # Return the plot\n    return plot\n\n\n\n\n\nSales are modeled using a stochastic Poisson process, where the expected rate \\(\\lambda_t\\) evolves over time.\nThe latent sales rate follows a random walk, allowing it to drift nonstationarily.\n\nSeasonal components (day-of-the-week and annual patterns) adjust for structured demand variations.\n\nPrice elasticity is explicitly modeled, ensuring sensitivity to pricing dynamics.\nThe model is implemented in numpyro, enabling scalable Bayesian inference.\n\nWe model the sales time series as a stochastic process where the underlying rate of sales evolves over time. This evolution follows a random walk structure, but with systematic adjustments for covariates such as price, day-of-the-week effects, and day-of-the-year effects. The rate of sales \\(\\lambda_t\\) on day \\(t\\) is a function of captures (i) systematic covariate effects (\\(z_t\\)), (ii) a global baseline (\\(\\mu_\\tau\\)), and (iii) the latent dynamic component (\\(\\tau_t\\)).\n\\[\nlog~\\lambda_t = z_t + \\mu_\\tau + \\tau_t\n\\]\n\nThe baseline sales level \\(\\tau_t\\) follows a random walk. Because all contrast matrices for structured effects are centered, \\(\\mu_\\tau + \\tau_t\\) can be interpreted as the average latent sales rate on \\(\\tau_t\\).\n\\[\n\\tau_t = \\tau_{t-1} + \\delta_t, \\quad \\delta_t \\sim \\mathcal{N}(0, \\sigma_\\tau)\n\\]\nwith:\n\\[\n\\mu_\\tau \\sim \\text{Exponential}(1), \\quad \\sigma_\\tau \\sim \\mathcal{N}(1)\n\\]\n\nWe further accounted for systematic effects of (i) day of the week, (ii) day of the year, and (iii) price.\n\nFor day of the week effects, we used a contrast matrix \\(\\mathbf{C}_{\\text{wday}}\\) with sliding differences.\nFor day of the year effects, we used a matrix of Gaussian radial basis functions \\(\\mathbf{B}_{\\text{yday}}\\).\nPrice elasticity is modelled using a centered log price\n\nSimilarly, the day-of-the-year effects are modeled using a seasonality basis matrix \\(\\mathbf{B}_{\\text{yday}}\\), which represents periodic seasonal patterns using Gaussian radial basis functions (RBFs).\n\n\nDay-of-the-week effects:\n\n\\[\n  zw_t = \\mathbf{C}_{\\text{wday}} \\cdot \\beta_{\\text{wday}}, \\quad \\beta_{\\text{wday}} \\sim \\mathcal{N}(0, 1)\n\\]\n\n\nDay-of-the-year effects:\n\n\\[\n  zy_t = \\mathbf{B}_{\\text{yday}} \\cdot \\beta_{\\text{yday}}, \\quad \\beta_{\\text{yday}} \\sim \\mathcal{N}(0, 1)\n\\]\n\n\nPrice elasticity:\n\n\\[\n  ze_t = \\text{log\\_price\\_centered} \\cdot e, \\quad \\log(-e) \\sim \\mathcal{N^{+}}(0, 1)\n\\]\n\n\nSum of structural effects:\n\n\\[\n  z_t = zw_t + zy_t + ze_t\n\\]\n\nObserved sales are assumed to follow a Poisson distribution, ensuring discrete, non-negative observations:\n\\[\nS_t \\sim \\text{Poisson}(\\lambda_t)\n\\]\n\nCodedef model_local_level_poisson(sales: jnp.array, log_price_centered: jnp.array, wday: jnp.array, yday_fraction: jnp.array, \n                              contrasts_sdif_t: jnp.array, contrasts_wday: jnp.array, contrasts_yday: jnp.array, \n                              downsampling_factor = 1):\n    \"\"\"\n    \"\"\"\n\n    n_obs = len(sales)\n    n_states = contrasts_sdif_t.shape[0]\n \n    def sample_random_walk(contrasts_sdif_t, n_states):\n        log_sigma = numpyro.sample(\"log_sigma\", dist.Gumbel(0, 5))\n        sigma = numpyro.deterministic(\"sigma\", jnp.exp(log_sigma))\n        log_state_mean = numpyro.sample(\"log_state_mean\", dist.Normal(0, 5)) # to-do: add an average drift term, as well as potentially an additional parameter governing drift dynamics \n        log_state_delta = numpyro.sample( \"log_state_delta\", dist.Normal(0, 1), sample_shape=(n_states-1,))\n        log_state_base = numpyro.deterministic(\"log_state_base\", jnp.dot(contrasts_sdif_t, log_state_delta) * sigma + log_state_mean )\n        return log_state_base\n\n    def sample_downsampled_random_walk(contrasts_sdif_t, n_obs, n_states):\n        log_state_base_downsampled = sample_random_walk(contrasts_sdif_t, n_states)\n        log_state_base = interpolate(log_state_base_downsampled, downsampling_factor, n_obs)\n        return log_state_base\n        \n    def sample_wday_effect(contrasts_wday, wday):\n        # Prior for day-of-the-week effects (6 coefficients)\n        wday_coefficients = numpyro.sample(\"wday_coefficients\", dist.Normal(0, 1), sample_shape=(6,))\n\n        # Compute wday effect per observation (sum-to-zero constraint applied via contrasts)\n        wday_effects = jnp.dot(contrasts_wday, wday_coefficients)\n        return jnp.array(wday_effects[wday-1])\n\n    def sample_yday_effect(contrasts_yday, yday_fraction):\n        # Prior for yearly seasonality effects (12 coefficients)\n        yday_coefficients = numpyro.sample(\"yday_coefficients\", dist.Normal(0, 1), sample_shape=(12,))\n        return jnp.dot(contrasts_yday, yday_coefficients)\n\n    def sample_price_effect(log_price_centered):\n        # Prior for price elasticity\n        elasticity_pos = numpyro.sample( \"elasticity_pos\", dist.HalfNormal(10) )\n        elasticity = numpyro.deterministic(\"elasticity\", -1 * elasticity_pos)\n        return log_price_centered * elasticity\n\n    # Sample random walk    \n    if n_obs == n_states:\n        log_state_base = sample_random_walk(contrasts_sdif_t, n_states)\n    else:\n        log_state_base = sample_downsampled_random_walk(contrasts_sdif_t, n_obs, n_states)\n\n    # Sample day-of-the-week effects\n    wday_effect = sample_wday_effect(contrasts_wday, wday)\n\n    # Sample day-of-the-year effects\n    yday_effect = sample_yday_effect(contrasts_yday, yday_fraction)\n\n    # Sample elasticity effect\n    price_effect = sample_price_effect(log_price_centered)\n\n    # Compute state\n    state = numpyro.deterministic(\"state\", jnp.exp( log_state_base  + yday_effect + wday_effect + price_effect )) #   # \n\n    # Compute log-likelihood for poisson emissions\n    numpyro.sample(\"sales\", dist.Poisson(rate=state), obs=sales) # to-do: create a Poisson distribution paramaterized by log-rate, as in the Stan manual"
  },
  {
    "objectID": "case-studies/time_series_analysis_1/ts_2_analysis.html#fit-the-model",
    "href": "case-studies/time_series_analysis_1/ts_2_analysis.html#fit-the-model",
    "title": "Bayesian Analysis of Sales",
    "section": "",
    "text": "We use the run_nuts function to fit the model to our sales data. The function leverages the No-U-Turn Sampler (NUTS) from the numpyro library to perform MCMC sampling. Because the model has a large number of latent parameters, initialization to sensible start values is key.\n\nIn order to fit the model, the functions below are used:\n\nprepare_model_arguments: Transforms the data into a format required by the model, including sales data, log-transformed prices, day-of-the-week values, and normalized day-of-the-year values.\ninit_values: Finds sensible start values for the model parameters which, in this case is crucial for the convergence of the MCMC algorithm.\nrun_nuts: Given a dataset, it calls the NUTS sampler to perform MCMC sampling.\n\n\nCodedef init_values(sales: jnp.array, log_price_centered: jnp.array, wday, yday_fraction: jnp.array, downsampling_factor = 1):\n    \"\"\"\n    \"\"\"\n    epsilon = 0.001\n    log_state_est = jnp.log(sales + epsilon)\n    log_state_mean_est = jnp.mean(log_state_est)\n    log_state_delta_est = jnp.diff(log_state_est)\n    if downsampling_factor &gt; 1:\n        log_state_delta_est = chunked_sum(log_state_delta_est, downsampling_factor)\n        \n    log_state_delta_sd_est = jnp.std(log_state_delta_est)\n\n    return {\n        \"log_sigma\": jnp.log( log_state_delta_sd_est ),\n        \"log_state_mean\": log_state_mean_est,\n        \"log_state_delta\": log_state_delta_est,\n        \"wday_coefficients\": jnp.array([0.0]*6),\n        \"yday_coefficients\": jnp.array([0.0]*12),\n        \"log_elasticity\": jnp.array([0.0])\n    }\n\n\n\nCodedef prepare_model_arguments(sales: jnp.array, log_price: jnp.array, wday: jnp.array, yday_fraction: jnp.array, downsampling_factor = 1):\n    \"\"\" \n    Prepares the arguments required for the model.\n    \n    Args:\n        sales: Array of sales data.\n        log_price: Array of log-transformed prices.\n        wday: Array of day-of-the-week values.\n        yday_fraction: Array of normalized day-of-the-year values.\n        downsampling_factor: Factor by which to downsample the data.\n    \n    Returns:\n        A tuple containing initialized values for the model parameters and the model arguments.\n    \"\"\"    \n    n_obs = len(sales)\n    \n    # Determine the number of states based on the downsampling factor\n    if downsampling_factor == 1:\n        n_states = n_obs\n    else:\n        n_states = int(np.ceil(n_obs / downsampling_factor) + 1)\n    \n    # Define contrast matrix for random walk (T coefficients, sum-to-zero constraint)\n    contrasts_sdif_t = patsy.contrasts.Diff().code_without_intercept(range(0, n_states)).matrix\n\n    # Define contrast matrix for day-of-the-week effects (6 coefficients, sum-to-zero constraint)\n    contrasts_wday = patsy.contrasts.Diff().code_without_intercept(range(0, 7)).matrix  # 7 days ‚Üí 6 contrasts\n\n    # Compute yday effect per observation (sum-to-zero constraint applied via contrasts)\n    contrasts_yday = compute_doy_basis(yday_fraction, sigma=30/365.25, n_centers=12) # to-do: do a very approximate calibration of the RBF width parameter sigma, using something like a spline for the long term trend + RBF seasonality\n\n    # Compute centered log price differences\n    log_price_centered = log_price - jnp.mean(log_price)\n\n    # Set up the model parameters\n    model_arguments = {\n        'sales': sales,\n        'log_price_centered': log_price_centered,\n        'wday': jnp.array(wday, dtype=int),\n        'yday_fraction': yday_fraction,\n        'downsampling_factor': downsampling_factor,\n        'contrasts_sdif_t': contrasts_sdif_t,\n        'contrasts_wday': contrasts_wday,\n        'contrasts_yday': contrasts_yday\n    }\n    \n    # Prepare initial values for parameters \n    init_params = init_values(sales, log_price_centered, wday, yday_fraction, downsampling_factor)\n\n    return init_params, model_arguments\n\n\n\nCodedef run_nuts(sales: jnp.array, log_price: jnp.array, wday, yday_fraction: jnp.array, downsampling_factor = 1, n_chains = 1, num_warmup=1_000, num_samples=1_000, step_size=0.01, max_tree_depth=8):\n    \"\"\" Runs NUTS MCMC inference on the model \n    \"\"\"\n    # Initialize random number generator key\n    rng_key = random.PRNGKey(0)\n    \n    # Get the number of observations\n    n_obs = len(sales)\n    \n    # Prepare model arguments and initial parameter values\n    init_params, model_arguments = prepare_model_arguments(sales = sales, log_price = log_price, wday = wday, yday_fraction = yday_fraction, downsampling_factor = downsampling_factor)\n\n    # Split the random number generator key\n    rng_key, rng_key_ = random.split(rng_key)\n\n    # Set the number of chains for parallel sampling\n    numpyro.set_host_device_count(n_chains)\n\n    # Define the model to be used\n    reparam_model = model_local_level_poisson\n    \n    # Initialize the NUTS kernel with the specified step size and tree depth\n    kernel = NUTS(reparam_model, step_size=step_size, max_tree_depth=max_tree_depth)\n    \n    # Initialize the MCMC sampler with the NUTS kernel\n    mcmc = MCMC(kernel, num_warmup=num_warmup, num_samples=num_samples, num_chains=n_chains)\n    \n    # Run the MCMC sampler\n    mcmc.run(rng_key_, **model_arguments) # disable init values: init_params=init_params\n\n    # Return the fitted MCMC object\n    return mcmc, model_arguments\n\n\n\n\nWe fit the model to the synthetic dataset using the run_nuts function. The model is fitted using the No-U-Turn Sampler (NUTS) from the numpyro library, with 4 chains, 1,000 warmup iterations, and 1,000 sampling iterations. The step size is set to 0.01, and the maximum tree depth is 8. The fitted model is stored in the m_fit variable.\nOn CPU, the process takes about 2 minutes.\n\n\nCode# read in the synthetic sales data\ndata = read_data(\"sales_synthetic.csv\")\n\n\n\nCodewith open(\"sim_parameters.pkl\", \"rb\") as f:\n    sim_parameters = date_range, growth, growth_plus_rw, scale_factor, wday, weekly_seasonality, yearly_seasonality = pickle.load(f)\n\n\n\nCode# Fit the model using NumPyro NUTS MCMC\nm_fit, model_arguments = run_nuts(data['sales'], data['log_price'], data['wday'], data['yday_fraction'], \n                                    downsampling_factor=7, n_chains=4, num_warmup=1_000, num_samples=1_000,\n                                    step_size=0.01, max_tree_depth=8)\n\n\n\n\nAll effective sample sizes are decent, which is a good sign. The Gelman-Rubin statistics are close to 1, indicating convergence. Inspection of trace plots are beyond the scope of this notebook.\nThe model successfully reconstructs all key features of the synthetic dataset.\n\n\n\nThe estimated random walk component closely follows the true trajectory of the combination long-term trend + random fluctuations in the synthetic data.\n\n\nCode# Let's look at the estimated random walk component of the model.\nsummary = az.summary(m_fit, var_names=[\"sigma\", \"log_state_delta\"], filter_vars=\"like\")\n\n\n\nCodepy$summary\n\n\n\nHere, we‚Äôll plot the estimated the random walk component, which also incorporates long term trends and growth or decline of sales. For present purposes, this is what amounts to irrelevant noise in the model.\n\n\nCode# Create a sequence of dates starting at data[\"date\"].min() the length of x['mean'], in steps of 7 days\nrw_states = az.summary(m_fit, var_names=[\"log_state_base\"], filter_vars=\"like\")[\"mean\"].to_numpy()\ndates = pd.date_range(start = data[\"date\"].min(), periods = len(rw_states), freq='7D')\n#p = plot_function(dates, np.exp(rw_states), \"Estimated Random Walk Component\", \"Date\", \"Sales\") # to-do: add uncertainty bands\n\ndf_1 = pl.DataFrame({'x': date_range, 'y': growth_plus_rw*scale_factor, 'var': 'Simulated Trend + Random Walk' })\ndf_2 = pl.DataFrame({'x': dates, 'y': np.exp(rw_states), 'var': 'Estimated Trend + Random Walk' })\ndf = pl.concat([df_1, df_2])\n\np = ggplot(df, aes(\"x\", \"y\", color = \"var\")) + geom_line() + theme_bw() + theme(legend_position='top') + guides(color=guide_legend(title=\"\"))\n_ = p.draw(show=True)\n\n\n\n\nThe model appears to correctly identifies weekly sales fluctuations.\n\n\nCodecoefs_wday = az.summary(m_fit, var_names=[\"wday_coefficients\"], filter_vars=\"like\")\n\n\n\nCodepy$coefs_wday\n\n\n\nCodewday_effect = jnp.dot(model_arguments[\"contrasts_wday\"], jnp.array(coefs_wday[\"mean\"]))\np = plot_function(range(0,7), wday_effect, \"Effect of Day of the Week\", \"Date\", \"Sales\") # to-do: add uncertainty bands\n\ndf_1 = pl.DataFrame({'x': range(0,7), 'y': weekly_seasonality - np.mean(weekly_seasonality), 'var': 'Simulated Weekly Seasonality' })\ndf_2 = pl.DataFrame({'x': range(0,7), 'y': wday_effect.tolist(), 'var': 'Estimated Weekly Seasonality' })\ndf = pl.concat([df_1, df_2])\n\np = ggplot(df, aes(\"x\", \"y\", color = \"var\")) + geom_line() + theme_bw() + theme(legend_position='top') + guides(color=guide_legend(title=\"\"))\n_ = p.draw(show=True)\n\n\n\n\nSeasonal peaks and troughs are largely accurately captured, with minor deviations.\n\n\nCodecoefs_yday = az.summary(m_fit, var_names=[\"yday_coefficients\"], filter_vars=\"like\")\n\n\n\nCodepy$coefs_yday\n\n\n\nCodeyday_effect = jnp.dot(model_arguments[\"contrasts_yday\"], jnp.array(coefs_yday[\"mean\"]))\n#p = plot_function(data[\"date\"], yday_effect, \"Yearly Seasonality\", \"Date\", \"Sales\") # to-do: add uncertainty bands\n\ndf_1 = pl.DataFrame({'x': date_range, 'y': yearly_seasonality - np.mean(yearly_seasonality), 'var': 'Simulated Yearly Seasonality' }).with_columns(\n    pl.col(\"x\").cast(pl.Date)\n)\ndf_2 = pl.DataFrame({'x': data[\"date\"].tolist(), 'y': yday_effect.tolist(), 'var': 'Estimated Yearly Seasonality' })\ndf = pl.concat([df_1, df_2])\n\np = ggplot(df, aes(\"x\", \"y\", color = \"var\")) + geom_line() + theme_bw() + theme(legend_position='top') + guides(color=guide_legend(title=\"\"))\n\n_ = p.draw(show=True)\n\n\n\n\nThe estimated elasticity coefficient is close enough to the true value (\\(-1.4\\)), though the \\(94\\%\\) credible interval is quite wide.\n\n\nCodesummary = az.summary(m_fit, var_names=[\"elasticity\"])\n\n\n\nCodepy$summary"
  },
  {
    "objectID": "case-studies/time_series_analysis_1/ts_2_analysis.html#conclusion",
    "href": "case-studies/time_series_analysis_1/ts_2_analysis.html#conclusion",
    "title": "Bayesian Analysis of Sales",
    "section": "",
    "text": "This case study demonstrates how Bayesian modeling can effectively decompose sales variance into meaningful components, providing a structured way to analyze the underlying factors driving sales fluctuations. By applying this approach to a synthetic dataset, we validated the model‚Äôs ability to separate out long-term growth, seasonal effects, and price sensitivity while simultaneously quantifying uncertainty.\nThe key takeaways include:\n\nDecomposing complexity: The model successfully isolates different components influencing sales, making it easier to interpret real-world dynamics.\nQuantifying uncertainty: In addition to point estimates, Bayesian inference provides full posterior distributions, enabling better risk assessment.\nInformed decision-making: By accounting for all sources of variance, businesses can make more confident strategic decisions that explicitly consider uncertainty. For instance, price optimization can be performed on the entire posterior distribution of the estimate of the price elasticity of demand.\n\nThese findings highlight the advantages of probabilistic modeling in sales analysis, offering a flexible and interpretable method."
  }
]